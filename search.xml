<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Review -- XGboost</title>
      <link href="/2019/12/02/ml-06/"/>
      <url>/2019/12/02/ml-06/</url>
      
        <content type="html"><![CDATA[<p>参考博客: <a href="https://cloud.tencent.com/developer/article/1513111" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1513111</a></p><h3 id="XGboost的目标函数"><a href="#XGboost的目标函数" class="headerlink" title="XGboost的目标函数"></a>XGboost的目标函数</h3><p>同大多数机器学习模型的目标函数一样, 包括训练损失和正则化项两个部分:</p><script type="math/tex; mode=display">obj=\sum_{i=1}^ml(y^{(i)},\hat{y}^{(i)})+\sum_{k=1}^K\Omega(f_k)</script><p>$l$ 是训练损失, 可以是平方损失, 也可以是逻辑回归损失. 后面的正则化项指出了树的复杂度. $K$ 表示树的数目, 注意: XGboost也是一个加法模型, 其对一个样本的预测得分是所有树的打分之和:</p><script type="math/tex; mode=display">\hat{y}^{(i)}=\sum_{k=1}^Kf_k(x^{(i)})</script><p>那么我们可以轻易地得到第 $t$ 次迭代的公式:</p><script type="math/tex; mode=display">\hat{y}^{(i)}_t=\sum_{k=1}^{t-1}f_k(x^{(i)})+f_t(x^{(i)})=\hat{y}^{(i)}_{t-1}+f_t(x^{(i)})</script><p>因此, 第 $t$ 次迭代的目标函数可以写成:</p><script type="math/tex; mode=display">\begin{aligned}obj_t&=\sum_{i=1}^ml(y^{(i)},\hat{y}_t^{(i)})+\sum_{k=1}^t\Omega(f_k) \\&=\sum_{i=1}^ml(y^{(i)},\hat{y}^{(i)}_{t-1}+f_t(x^{(i)}))+\sum_{k=1}^t\Omega(f_k) \\&=\sum_{i=1}^ml(y^{(i)},\hat{y}^{(i)}_{t-1}+f_t(x^{(i)}))+\Omega(f_t) + \sum_{k=1}^{t-1}\Omega(f_k) \\&=\sum_{i=1}^ml(y^{(i)},\hat{y}^{(i)}_{t-1}+f_t(x^{(i)}))+\Omega(f_t) + \text{const} \\\end{aligned}</script><p>上式中唯一的变量就是第 $t$ 棵树 $f_t$, 其它的例如:</p><script type="math/tex; mode=display">y^{(i)}\text{是标签, 是一个常量.} \\\hat{y}^{(i)}_{t-1}\text{是前}t-1{棵树的预测打分, 也是已经计算出来的结果, 是一个常量.}  \\\sum_{k=1}^{t-1}\Omega(f_k)\text{是前}t-1{棵树的复杂度, 已经构造完毕, 是一个常量.} \\</script><blockquote><blockquote><p>泰勒公式的二阶展开式</p><script type="math/tex; mode=display">f(x + \Delta x)\approx f(x) + f'(x)\Delta x + \frac{1}{2}f''(x)\Delta x^2</script></blockquote></blockquote><p>看一下 $l(y^{(i)},\hat{y}^{(i)}<em>{t-1}+f_t(x^{(i)}))$, $y^{(i)}$ 是一个常量, $\hat{y}^{(i)}</em>{t-1}$ 相当于泰勒展开式中的 $x$, $f_t(x^{(i)})$ 相当于泰勒展开式中的 $\Delta x$, 因此</p><script type="math/tex; mode=display">\begin{aligned}l(y^{(i)},\hat{y}^{(i)}_{t-1}+f_t(x^{(i)}))&=l(y^{(i)},\hat{y}^{(i)}_{t-1})+\frac{\partial{l}}{\partial{\hat{y}^{(i)}_{t-1}}}f_t(x^{(i)})+\frac{1}{2}\frac{\partial{l}}{\partial^2{\hat{y}^{(i)}_{t-1}}}f^2_t(x^{(i)}) \\&=l(y^{(i)},\hat{y}^{(i)}_{t-1})+g^{(i)}f_t(x^{(i)})+\frac{1}{2}h^{(i)}f^2_t(x^{(i)})\end{aligned}</script><p>其中,</p><script type="math/tex; mode=display">g^{(i)}=\frac{\partial{l}}{\partial{\hat{y}^{(i)}_{t-1}}} \\h^{(i)}=\frac{\partial{l}}{\partial^2{\hat{y}^{(i)}_{t-1}}}</script><p>代回原来的目标函数:</p><script type="math/tex; mode=display">\begin{aligned}obj_t&=\sum_{i=1}^ml(y^{(i)},\hat{y}_t^{(i)})+\sum_{k=1}^t\Omega(f_k) \\&=\sum_{i=1}^m[l(y^{(i)},\hat{y}^{(i)}_{t-1})+g^{(i)}f_t(x^{(i)})+\frac{1}{2}h^{(i)}f^2_t(x^{(i)})]+\Omega(f_t) + \text{const} \\\end{aligned}</script><p>去掉所有不影响优化的常量后, 得到最终的目标函数为:</p><script type="math/tex; mode=display">\begin{aligned}obj_t&=\sum_{i=1}^m[g^{(i)}f_t(x^{(i)})+\frac{1}{2}h^{(i)}f^2_t(x^{(i)})]+\Omega(f_t) \\\end{aligned}</script><h3 id="定义XGboost中的树结构"><a href="#定义XGboost中的树结构" class="headerlink" title="定义XGboost中的树结构"></a>定义XGboost中的树结构</h3><h4 id="树的定义式"><a href="#树的定义式" class="headerlink" title="树的定义式"></a>树的定义式</h4><ul><li>叶子结点的权重向量 $w$ ;</li><li>样本到叶子节点的映射关系 $q$, 假设一棵树有 $N$ 个叶子节点.</li></ul><p>其中,</p><script type="math/tex; mode=display">q: \mathbb{R}^d\rightarrow\{1,2,...,N\}</script><p>在此基础上, 一棵树可以表达为:</p><script type="math/tex; mode=display">f_t(x)=w_{q(x)}</script><p>比如, 假设有一棵树, 它有3个叶子结点, 叶子结点的权重向量为 $[2, 0.1, -1]^T$ , 给定样本 $x^{(1)}$, 它被分到了第一个叶子结点上, 那么 $q(x^{(1)})=1$, 从而得到打分为:</p><script type="math/tex; mode=display">f_t(x^{(1)})=w_{q(x)}=w_1=2</script><h4 id="树的复杂度"><a href="#树的复杂度" class="headerlink" title="树的复杂度"></a>树的复杂度</h4><ul><li>叶子结点的数量;</li><li>叶子结点的权重向量的 $L_2$ 范数.</li></ul><p>因此</p><script type="math/tex; mode=display">\Omega(f_t)=\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2</script><p>如果还是拿上面的例子, 一棵3个叶子结点的树的叶子权重向量为 $[2, 0.1, -1]^T$ , 那么它的复杂度为:</p><script type="math/tex; mode=display">\Omega=3\gamma + \frac{1}{2}\lambda(4+0.01+1)=3\gamma+2.505\lambda</script><h4 id="叶子结点分组"><a href="#叶子结点分组" class="headerlink" title="叶子结点分组"></a>叶子结点分组</h4><p>叶子结点分组就是将划分到同一个叶子节点上的所有样本归类到同一个样本子集上, 也就是说<strong>一个叶子结点和一个样本子集一一对应</strong>, 如果用 $I_j$ 表示第 $j$ 个叶子节点对应的样本子集, 那么:</p><script type="math/tex; mode=display">I_j=\{i|q(x^{(i)})=j\}</script><p>由于分组后各个叶子结点上的样本数的总和其实就是数据集的大小, 所以:</p><script type="math/tex; mode=display">\sum_{i=1}^m=\sum_{j=1}^N\sum_{i \in I_j}</script><p>代回原来的目标函数:</p><script type="math/tex; mode=display">\begin{aligned}obj_t&=\sum_{i=1}^m[g^{(i)}f_t(x^{(i)})+\frac{1}{2}h^{(i)}f^2_t(x^{(i)})]+\Omega(f_t) \\&=\sum_{i=1}^m[g^{(i)}w_{q(x^{(i)})}+\frac{1}{2}h^{(i)}w^2_{q(x^{(i)})}]+\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2 \\&=\sum_{j=1}^N\sum_{i \in I_j}[g^{(i)}w_{q(x^{(i)})}+\frac{1}{2}h^{(i)}w^2_{q(x^{(i)})}]+\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2 \\&=\sum_{j=1}^N[\sum_{i \in I_j}g^{(i)}w_{q(x^{(i)})}+\frac{1}{2}\sum_{i \in I_j}h^{(i)}w^2_{q(x^{(i)})}]+\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2 \\&=\sum_{j=1}^N[\sum_{i \in I_j}g^{(i)}w_j+\frac{1}{2}\sum_{i \in I_j}h^{(i)}w^2_j]+\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2 \\&=\sum_{j=1}^N[(\sum_{i \in I_j}g^{(i)})w_j+\frac{1}{2}(\sum_{i \in I_j}h^{(i)})w^2_j]+\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2 \\&=\sum_{j=1}^N[G^{(j)}w_j+\frac{1}{2}H^{(j)}w^2_j]+\gamma N+\frac{1}{2}\lambda\sum_{j=1}^Nw_j^2 \\\end{aligned}</script><p>其中</p><script type="math/tex; mode=display">G^{(j)}=\sum_{i \in I_j}g^{(i)} \\H^{(j)}=\sum_{i \in I_j}h^{(i)} \\</script><p>分别代表叶子结点 $j$ 的所有样本的一阶偏导数之和与二阶偏导数之和. 进一步整理可以得到:</p><script type="math/tex; mode=display">obj_t = \sum_{j=1}^N[G^{(j)}w_j+\frac{1}{2}(H^{(j)}+\lambda)w^2_j]+\gamma N</script><h4 id="树结构打分"><a href="#树结构打分" class="headerlink" title="树结构打分"></a>树结构打分</h4>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集成学习</title>
      <link href="/2019/11/27/ml-05/"/>
      <url>/2019/11/27/ml-05/</url>
      
        <content type="html"><![CDATA[<p>根据个体学习器的生成方式，目前的集成学习大致可以分为两类：</p><ul><li>个体学习器之间存在强依赖关系, 必须串行生成的序列化方法, 如Boosting;</li><li>个体学习器之间不存在强依赖关系, 可同时生成的并行化方法, 如随机森林和Bagging.</li></ul><p>集成学习最简单的方法是简单的投票法, 假设$f$ 为真实的函数, 如果假设基分类器的错误率为 $\epsilon$, 那么对于每个基分类器 $h_i$ 有(相当于计算频率):</p><script type="math/tex; mode=display">P(h_i(x)\neq f(x))=\epsilon</script><p>基于简单投票法结合 $T$ 个分类器, 若有超过半数的基分类器正确, 则集成分类就是正确的:</p><script type="math/tex; mode=display">H(x)=\text{sign}\big(\sum_{i=1}^Th_i(x)\big)</script><h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p>Boosting族最著名的方法是AdaBoost, 它有多种推导的方式，以”加性模型”(基学习器的线性组合)为例, 设</p><script type="math/tex; mode=display">y^{(i)}\in\{-1,1\}</script><p>那么基学习器的线性组合为:</p><script type="math/tex; mode=display">H(x)=\sum_{i=1}^T\alpha_th_t(x)</script><blockquote><blockquote><p>条件数学期望:</p><script type="math/tex; mode=display">\mathbb{E}_{x\sim P_1,y\sim P_2}[y|x]=\sum_yy\cdot p(y|x) \\\mathbb{E}_{x\sim P_1,y\sim P_2}[g(y)|x]=\sum_yg(y)\cdot p(y|x)</script></blockquote></blockquote><p>我们需要最小化经验/训练指数损失（与泛化损失相对, 此处 $D$ 是一个数据/概率分布, 显然 $x \sim D$, 真实函数 $f(x)$ 相当于前面条件期望中的 $y$, 取值为 $-1$ 和 $1$, 服从一个二项分布, 在公式中省略不写, 只写出 $X \sim D$):</p><script type="math/tex; mode=display">\begin{aligned}l_{exp}(H|D)&=\mathbb{E}_{x\sim D}[e^{-f(x)H(x)}|x] \\&=\sum_{f(x)}p(f(x)|x)e^{-f(x)H(x)} \\&=p(f(x)=1|x)e^{-H(x)}+p(f(x)=-1|x)e^{H(x)}\end{aligned}</script><p>可以首先对指数损失求偏导数并置0:</p><script type="math/tex; mode=display">\frac{\partial{l_{exp}(H|D)}}{\partial{H(x)}}=-p(f(x)=1|x)e^{-H(x)}+p(f(x)=-1|x)e^{H(x)}=0</script><p>也就是</p><script type="math/tex; mode=display">H(x)=\frac{1}{2}\ln\frac{p(f(x)=1|x)}{p(f(x)=-1|x)}</script><p>因此, 有</p><script type="math/tex; mode=display">\begin{aligned}\text{sign}(H(x))&=\text{sign}(\frac{1}{2}\ln\frac{p(f(x)=1|x)}{p(f(x)=-1|x)}) \\&=\begin{cases}1, & p(f(x)=1|x) > p(f(x)=-1|x) \\-1, & p(f(x)=1|x) < p(f(x)=-1|x)\end{cases} \\&=\mathop{\arg\min}_{y \in \{-1, 1\}}P(f(x)=y|x)\end{aligned}</script><p>在Adaboost中, 第一个基分类器 $h_1$ 是使用原始数据分布生成的, 此后迭代生成 $h_t$ 和 $\alpha_t$, <strong>当基分类器 $h_t$ 基于分布 $D_t$ 产生后, 该基分类器的权重应该使得 $\alpha_th_t$ 最小化损失函数</strong>, 从数学期望(理想的角度)来看, $h_t(x)$ 是可以取到 $+1$ 或者 $-1$ 的. (前面的是让 $H(x)=\sum_t\alpha_th_t$ 使得损失最小化)</p><script type="math/tex; mode=display">l_{exp}(\alpha_th_t|D_t)=E_{x\sim D_t}[e^{-f(x)\alpha_th_t(x)}|x]</script><p>由于</p><script type="math/tex; mode=display">f(x)h_t(x)=\begin{cases}1, & f(x)=h_t(x) \\-1, & f(x) \neq h_t(x) \\\end{cases}</script><p>所以</p><script type="math/tex; mode=display">e^{-\alpha_tf(x)h_t(x)}=\begin{cases}e^{-\alpha_t}, & f(x)=h_t(x) \\e^{\alpha_t}, & f(x) \neq h_t(x) \\\end{cases}</script><p>可以用指示函数将上面的公式写成:</p><script type="math/tex; mode=display">e^{-\alpha_tf(x)h_t(x)}=e^{-\alpha_t}\cdot\mathbb{I}(f(x)=h_t(x))+e^{\alpha_t}\cdot\mathbb{I}(f(x)\neq h_t(x))</script><blockquote><blockquote><p>一般情况下,</p><script type="math/tex; mode=display">y=\begin{cases}a && \text{p} \\b && \lnot\text{p}\end{cases}=a\cdot\mathbb{I}(\text{p})+b\cdot\mathbb{I}(\lnot\text{p})</script></blockquote></blockquote><p>因此</p><script type="math/tex; mode=display">\begin{aligned}l_{exp}(\alpha_th_t|D_t)&=\mathbb{E}_{x\sim D_t}[e^{-f(x)\alpha_th_t(x)}|x] \\&=\mathbb{E}_{x\sim D_t}[e^{-\alpha_t}\cdot\mathbb{I}(f(x)=h_t(x))+e^{\alpha_t}\cdot\mathbb{I}(f(x)\neq h_t(x))] \\&=e^{-\alpha_t}P_{x\sim D_t}(f(x)=h_t(x)|x)+e^{\alpha_t}P_{x\sim D_t}(f(x)\neq h_t(x)) \\&=e^{-\alpha_t}(1-\epsilon_t)+e^{\alpha_t}\epsilon_t \\\end{aligned}</script><p>我们前面已经定义过分类误差: $P_{x\sim D_t}(h_t(x)\neq f(x))=\epsilon_t$, 求指数函数损失对权重的导数并置0:</p><script type="math/tex; mode=display">\frac{\partial{l_{exp}(\alpha_th_t|D_t)}}{\partial(\alpha_t)}=-e^{-\alpha_t}(1-\epsilon_t)+e^{\alpha_t}\epsilon_t=0</script><p>求得权重的更新公式</p><script type="math/tex; mode=display">\alpha_t=\frac{1}{2}\ln\big(\frac{1-\epsilon_t}{\epsilon_t}\big)</script><p>在Adaboost算法中, (无论是否施加权重), 下一轮的基分类器 $h_t$ 都能纠正之前的基分类器犯下的一些错误, 理想情况下应该可以纠正全部错误, 也就是最小化 (注意到理想情况下 $f^2(x)=h^2(x)=1$)</p><script type="math/tex; mode=display">\begin{aligned}l_{exp}(H_{t-1}+h_t|D)&=\mathbb{E}_{x\sim D}[e^{-f(x)(H_{t-1}(x)+h_t(x))}|x] \\&=\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}e^{-f(x)h_t(x)}|x] \\&\approx\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}\big(1-f(x)h_t(x)+\frac{f^2(x)h_t^2(x)}{2}\big)|x] \\&=\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}\big(1-f(x)h_t(x)+\frac{1}{2}\big)|x]\end{aligned}</script><blockquote><blockquote><p>注意 $e^x$ 的泰勒展开式为: $e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+…+\frac{x^n}{n!}+….$</p></blockquote></blockquote><p>所以理想的基学习器为:</p><script type="math/tex; mode=display">\begin{aligned}h_t(x)&=\mathop{\arg\min}_hl_{exp}(H_{t-1}+h|D) \\&=\mathop{\arg\min}_h\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}\big(1-f(x)h_t(x)+\frac{1}{2}\big)|x] \\&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}f(x)h_t(x)|x] \\&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D}\big[\frac{e^{-f(x)H_{t-1}(x)}f(x)h_t(x)}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}|x\big] \\&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D}\big[\frac{e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h_t(x)|x\big] \\&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D}\big[\frac{e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h_t(x)\big]\end{aligned}</script><p>上面公式的最后一行和倒数第二行的区别在于最后一行不把 $f(x)$ 当成随机变量（二项分布）, 而当成 $x$ 的函数, 而倒数第二行相当于把 $f(x)$ 当成了另外一个随机变量, 比如假设为 $y$, 因此前者(倒数第一行)是一般的数学期望, 后者是条件数学期望, 理解方式不同但它们是可以等价的, 注意到:</p><script type="math/tex; mode=display">\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]</script><p>是一个常数, 令 $D_t$ 表示一个分布:</p><script type="math/tex; mode=display">D_t(x)=\frac{D(x)e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}</script><p>根据数学期望的定义, 等价于:</p><script type="math/tex; mode=display">\begin{aligned}h_t(x)&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D}\big[\frac{e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h_t(x)\big] \\&=\mathop{\arg\max}_h\sum_{x}D(x)\frac{e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}f(x)h_t(x) \\&=\mathop{\arg\max}_h\sum_{x}D_t(x)f(x)h_t(x) \\&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D_t}[f(x)h_t(x)]\end{aligned}</script><p>理想情况下, 有</p><script type="math/tex; mode=display">\begin{aligned}f(x)h(x)&=1\cdot\mathbb{I}(f(x)=h_t(x))+(-1)\cdot\mathbb{I}(f(x)\neq h_t(x)) \\&=1-\mathbb{I}(f(x)\neq h_t(x))+(-1)\cdot\mathbb{I}(f(x)\neq h_t(x)) \\&=1-2\mathbb{I}(f(x)\neq h_t(x))\end{aligned}</script><p>所以又有:</p><script type="math/tex; mode=display">\begin{aligned}h_t(x)&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D_t}[f(x)h_t(x)] \\&=\mathop{\arg\max}_h\mathbb{E}_{x\sim D_t}[1-2\mathbb{I}(f(x)\neq h_t(x))] \\&=\mathop{\arg\min}_h\mathbb{E}_{x\sim D_t}[\mathbb{I}(f(x)\neq h_t(x))]\end{aligned}</script><p>可以看到理想的 $h_t$ 在分布 $D_t$ 下最小化分类误差, 弱分类器将基于分布 $D_t$ 进行训练, 且分类误差要小于 $0.5$. 接下来考虑前后两次迭代的数据分布的关系:</p><script type="math/tex; mode=display">\begin{aligned}D_{t+1}(x)&=\frac{D(x)e^{-f(x)H_{t}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t}(x)}]} \\&=\frac{D(x)e^{-f(x)(H_{t-1}(x)+\alpha_th_t(x))}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t}(x)}]} \\&=\frac{D(x)e^{-f(x)H_{t-1}(x)}e^{-f(x)\alpha_th_t(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t}(x)}]} \\&=\frac{D(x)e^{-f(x)H_{t-1}(x)}}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}\cdot \frac{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t}(x)}]} \cdot e^{-f(x)\alpha_th_t(x)} \\&= D_t(x)\cdot e^{-f(x)\alpha_th_t(x)} \cdot \frac{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t}(x)}]}\end{aligned}</script><p>定义规范化因子:</p><script type="math/tex; mode=display">\begin{aligned}Z_t&=\frac{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t-1}(x)}]}{\mathbb{E}_{x\sim D}[e^{-f(x)H_{t}(x)}]} \\&=\frac{\sum_{i=1}^me^{-f(x^{(i)})H_{t-1}(x^{(i)})}}{\sum_{i=1}^me^{-f(x^{(i)})H_{t}(x^{(i)})}}\end{aligned}</script><p>所有东西理清楚后, 我们再来看一下算法的伪代码 (初始分布为均匀分布, 由于算法截取自西瓜书, 其样本表示为 $x_i$ 而不是 $x^{(i)}$)，如下所示 (核心在于 $\alpha_t$ 和 $D_t$ 的更新公式):</p><p><img src="/img/Adaboost.png" style="height:450px;width:600px;"></p><p>从偏差-方差的角度来看, <strong>Boosting主要关注降低偏差, 因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成</strong>.</p><h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>要使泛化性能得到更强的集成，个体学习器应该尽可能相互独立, 为了让个体学习器之间的差异拉开，一种可行的做法是对原始数据集进行采样, 产生多个不同的数据子集，再从每个子集中各训练出一个基学习器, bagging算法就是基于这样的思路进行设计的.</p><p>与Boosting不同, Bagging是一种并行化的集成学习算法, 算法的关键点之一在于自助采样, 所谓自助采样, 指的是在给定 $m$ 个样本的数据集中先随机抽出 $m$ 个样本放入采样集中, 再把该样本放回初始数据集, 这样下次采样时该样本仍然有可能被选中, 经过 $m$ 次随机采样, 可以产生一个 $m$ 个样本的数据子集, 有的样本可能重复出现, 假设每个样本被采样的概率为 $\frac{1}{m}$, 那么一个样本经过 $m$ 次采样都被抽中的概率为 $(1-\frac{1}{m})^m$, 即:</p><script type="math/tex; mode=display">\lim_{m\rightarrow +\infty}(1-\frac{1}{m})^m=\frac{1}{e}\approx0.368</script><p>这也就是说原始训练集中约有 $63.2\%$ 的样本会出现在采样集中. Bagging的大致流程为:</p><ul><li>采样 $T$ 个含有 $m$ 个样本的采样集;</li><li>基于每个采样集训练出一个基学习器, 再进行结合 (分类用简单投票, 回归用简单平均).<br><img src="/img/Bagging.png" style="height: 240px; width: 520px;"></li></ul><p>比如以二分类为例, 就是</p><script type="math/tex; mode=display">H(x)=\mathop{\arg\max}_{\{0,1\}}\big\{\sum_{t=1}^T\mathbb{I}(h_t(x)=0), \sum_{t=1}^T\mathbb{I}(h_t(x)=1)\big\}</script><p>与标准的AdaBoost只只用于二分类任务不同, Bagging能不经修改地用于多分类和回归任务. 从偏差-方差的角度看, <strong>Bagging主要关注降低方差, 因此它在不剪枝决策树, 神经网络等易受样本扰动的学习器上效用更为明显.</strong></p><h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>随机森林RF是 <strong>Bagging的一个扩展变体</strong>. 其特点在于:</p><ul><li>以决策树为基学习器构建Bagging集成;</li><li>在决策树训练过程中引入随机属性选择.</li></ul><p>随机森林的引入随机属性和传统决策树的区别在于:</p><ul><li>传统决策树是在当前结点的属性集合中(设有 $d$ 个属性)中选择一个最优属性;</li><li>RF中则是从基决策树的每个结点的属性集合中随机选择一个包含 $k$ 的属性子集, 在从子集中挑选出一个最优属性来划分, 一般情况下, 推荐 $k = \log_2d$.</li></ul><p>随机森林对比Bagging的优势在于:</p><ul><li>不仅有样本扰动 (自助采样), 还有属性扰动, 提高了最终的泛化性能;</li><li>训练效率高, bagging用”确定型”决策树为基学习器时要对结点的全部属性进行考察, 而RF使用的”随机型”决策树则只需要考察一个属性子集.</li></ul><h3 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h3><h4 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h4><ul><li>简单平均法: $H(x)=\frac{1}{T}\sum_{i=1}^Th_i(x)$</li><li>加权平均法: $H(x)=\sum_{i=1}^Tw_ih_i(x)$</li></ul><p>后者的权重参数 $w_i$ 需要学习, 如果集成规模大, 参数变多, 会引发过拟合, 一般而言, 个体学习器性能差异较大时宜使用加权平均, 否则简单平均更好.</p><h4 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h4><ul><li>绝对多数投票法</li></ul><p>给定类别标记</p><script type="math/tex; mode=display">\{c_1, c_2,..., c_N\}</script><p>基学习器 $h_i$ 预测出来的结果 (投票数) 为:</p><script type="math/tex; mode=display">(h_i^1(x), h_i^2(x), ..., h_i^N(x))</script><p>当一个所有基学习器在某个类别上的总投票超过所有的票数的一半是则达成预测, 否则拒绝预测:</p><script type="math/tex; mode=display">H(x)=\left\{\begin{array}{ll}{c_{j},} & {\text { if } \sum_{i=1}^{T} h_{i}^{j}(x)>0.5 \sum_{k=1}^{N} \sum_{i=1}^{T} h_{i}^{k}(x)} \\ {\text { reject, }} & {\text { otherwise. }}\end{array}\right.</script><ul><li><p>相对多数投票法</p><script type="math/tex; mode=display">H(x)=c_{\arg \max } \sum_{i=1}^{T} h_{i}^{j}(x)</script><p>缺点在于不提供拒绝预测的选项, 是绝对多数投票的退化版.</p></li><li><p>加权投票法</p><script type="math/tex; mode=display">H(x)=c_{\arg \max } \sum_{i=1}^{T} w_{i} h_{i}^{j}(x)</script><p>其中 $w_i \geq 0$, $\sum_iw_i=1$.</p></li></ul><p>$h_i^j(x)$ 的值可以是类标记 ${0,1}$, 也可以是类概率 $[0, 1]$ (后验概率 $P(c_j|x)$).</p><h4 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h4><p>当训练数据比较多时, 可以使用另外一种强大的结合策略, 称为”学习法”, 学习法中的个体学习器称为初级学习器, 用于结合的学习器称为次级学习器. 学习法的代表是stacking算法, 其主要思路是:</p><ul><li>从初始训练集中训练出初级学习器;</li><li>初级学习器的输出作为新样本的特征, 初级学习器输入的样本的标签作为新样本的标签, 构建新数据集并训练次级学习器.</li></ul><p>算法的伪代码如下:</p><p><img src="/img/stacking.png" style="width=220px;height:460px;"></p><p>为了减少过拟合的风险, 可以将初始训练集 $D$ 划分为 $k$ 折 $D_1, D_2, …, D_k$,</p><ul><li>令测试集为第 $j$ 折测试集 $D_j$</li><li>令训练集为 $\overline{D}_j=D-D_j$</li></ul><p>第 $t$ 个初级学习器在 $\overline{D}_j$ 上学习得到,<br>对</p><script type="math/tex; mode=display">\forall x^{(i)} \in D_j</script><p>令</p><script type="math/tex; mode=display">z^{(i)}_t=h_t^{(j)}(x^{(i)})</script><p>得到新数据集的样本特征为</p><script type="math/tex; mode=display">z^{(i)}=\{z^{(i)}_{1};z^{(i)}_{2};...;z^{(i)}_{T}\}</script><p>样本标签依旧为 $y^{(i)}$, 交叉训练结束后得到完整的次级训练集为</p><script type="math/tex; mode=display">D'=\{(z^{(i)},y^{(i)})\}_{i=1}^m</script><p>然后 $D’$ 用于训练次级学习器.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/2019/11/18/ml-04/"/>
      <url>/2019/11/18/ml-04/</url>
      
        <content type="html"><![CDATA[<h3 id="决策树基础"><a href="#决策树基础" class="headerlink" title="决策树基础"></a>决策树基础</h3><p>关于决策树，需要注意如下几点:</p><ul><li>决策树的非叶子结点对应一个属性测试</li><li>决策树的叶子结点对应一个决策结果</li><li>决策树的每个结点都存放着样本集的一个子集，代表划分到这个结点上的样本有多少个.</li></ul><p>决策树的生成算法采用简单直观的”分而治之”策略：<br><img src="/img/decisionTree.png" style="height:450px;width:630px;"></p><p>从算法的伪代码中可以看出递归结束的三个条件:</p><ul><li><strong>当前结点</strong>所包含的样本全部属于同一类别C，<strong>无需划分</strong>, 当前结点标记为叶结点, 类别标记为<strong>C</strong>;</li><li><strong>当前属性集</strong>为空, 或者所有样本在所有属性上取值相同，<strong>无法划分</strong>, 当前结点标记为叶节点, 类别标记为<strong>该节点的多数类</strong>;</li><li><strong>当前结点</strong>包含的样本集合为空，<strong>无法划分</strong>, 当前结点标记为叶节点, 类别标记为<strong>该节点父节点的多数类</strong>.</li></ul><h3 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h3><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>从决策树的生成算法种可以看出，如何选择最优划分属性是比较关键的一步. “信息熵”通常可以用来度量样本集合的纯度/不确定性, 假设当前样本集合 $D$ 中第 $k$ 类样本所占的比例为为 $p_k (k=1,2,…,K)$, 那么 $D$ 的信息熵定义为:</p><script type="math/tex; mode=display">\text{Ent}(D)=-\sum_{k=1}^Kp_k\log_2p_k</script><p>$\text{Ent}(D)$ 的值越小, 则 $D$ 的不确定性也越小，纯度就越高. 如下是出自西瓜书的一份西瓜数据集:<br><img src="/img/watermelon.png" style="height:450px;width:560px;"></p><p>决策树的根节点存放着整个样本数据集 $D$，由于数据集中只存在两个类别，也就是 $K=2$ ,从表格中可以得知正类所占据的比例为: $p_1=\frac{8}{17}$, 负类所占据的比例为: $p_2=\frac{9}{17}$, 所以 $D$ 的信息熵为:</p><script type="math/tex; mode=display">\text{Ent}(D)=-\sum_{k=1}^2p_k\log_2p_k=-(\frac{8}{17}\log_2\frac{8}{17}+\frac{9}{17}\log_2\frac{9}{17})=0.998</script><p>假设离散属性 $a$ 有 $V$ 个不同的取值：</p><script type="math/tex; mode=display">\{a^1, a^2, ..., a^V\}</script><p>若使用 $a$ 的上述 $V$ 个不同的取值对 $D$ 进行划分, 那么可以产生如下 $V$ 个划分集, 或者说产生 $V$ 个分支结点:</p><script type="math/tex; mode=display">\{D^1, D^2, ..., D^V\}</script><p>并赋予这 $V$ 个分支结点相关权重为: $\frac{|D^v|}{|D|}$, 表示样本数越多的分支结点的影响越大. 据此定义处属性 $a$ 对样本集 $D$ 进行划分所得到的信息增益:</p><script type="math/tex; mode=display">\text{Gain}(D,a)=\text{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Ent}(D^v)</script><p>一般而言，信息增益越大，说明使用属性 $a$ 来进行划分所获得纯度提升越大，不确定性降得越小:</p><script type="math/tex; mode=display">a_*=\arg\max_{a \in A}\text{Gain}(D,a)</script><p>著名的<strong>ID3算法</strong>使用的就是信息增益为准则来划分属性的. 可以看到, 西瓜数据集的初始属性集为:</p><script type="math/tex; mode=display">A=\{\text{色泽}, \text{根蒂}, \text{敲声}, \text{纹理}, \text{脐部}, \text{触感}\}</script><p>也就是我们要分别计算这6个属性的信息增益, 然后从中挑选出信息增益最大的那个. 例如假设当前待计算的属性 $a$ 为色泽 $(V=3)$, 那么:</p><script type="math/tex; mode=display">a^1=青绿,a^2=乌黑,a^3=浅白</script><p>对应划分出3个子数据集为:</p><script type="math/tex; mode=display">\begin{aligned}&D^1=\{1,4,6,10,13,17\},|D^1|=6,p_1=\frac{3}{6}, p_2=\frac{3}{6}\\&D^2=\{2,3,7,8,9,15\},|D^2|=6,p_1=\frac{4}{6},p_2=\frac{2}{6}\\&D^3=\{5,11,12,14,16\},|D^3|=5,p_1=\frac{1}{5},p_2=\frac{4}{5} \\\end{aligned}</script><p>于是，这3个数据子集的信息熵为:</p><script type="math/tex; mode=display">\text{Ent}(D^1)=-\sum_{k=1}^2p_k\log_2p_k=-(\frac{3}{6}\log_2\frac{3}{6}+\frac{3}{6}\log_2\frac{3}{6})=1.000 \\\text{Ent}(D^2)=-\sum_{k=1}^2p_k\log_2p_k=-(\frac{4}{6}\log_2\frac{4}{6}+\frac{2}{6}\log_2\frac{2}{6})=0.918 \\\text{Ent}(D^3)=-\sum_{k=1}^2p_k\log_2p_k=-(\frac{1}{5}\log_2\frac{1}{5}+\frac{4}{5}\log_2\frac{4}{5})=0.722</script><p>由此可以得出属性”色泽”的信息增益为:</p><script type="math/tex; mode=display">\begin{aligned}\text{Gain}(D,\text{色泽})&=\text{Ent}(D)-\sum_{v=1}^3\frac{|D^v|}{|D|}\text{Ent}(D^v) \\&=0.998-(\frac{6}{17}\times1.000+\frac{6}{17}\times0.918+\frac{5}{17}\times0.722) \\&=0.109\end{aligned}</script><p>同理，我们可以得到其它5个属性的信息增益为:</p><script type="math/tex; mode=display">\text{Gain}(D,\text{根蒂})=0.143 \\\text{Gain}(D,\text{敲声})=0.141 \\\text{Gain}(D,\text{纹理})=0.381 \\\text{Gain}(D,\text{脐部})=0.289 \\\text{Gain}(D,\text{触感})=0.006 \\</script><p>那么最优划分属性为:</p><script type="math/tex; mode=display">a_*=\arg\max_{a \in A}\text{Gain}(D,a)=\text{纹理}</script><p>划分完后我们可以得到如下的一棵树:<br><img src="/img/Treedivide1.png" style="height:150px;width:550px;"></p><p>然后，决策树学习算法可以对每一个分支结点做进一步划分，比如以分支结点 $D^1$ (“纹理=清晰”)为例，可用属性集:</p><script type="math/tex; mode=display">A/\{纹理\}=\{\text{色泽}, \text{根蒂}, \text{敲声}, \text{脐部}, \text{触感}\}</script><p>接着，基于 $D^1$ 计算出各个属性的信息增益:</p><script type="math/tex; mode=display">\text{Gain}(D^1,\text{色泽})=0.043 \\\text{Gain}(D^1,\text{根蒂})=0.458 \\\text{Gain}(D^1,\text{敲声})=0.331 \\\text{Gain}(D^1,\text{脐部})=0.458 \\\text{Gain}(D^1,\text{触感})=0.458 \\</script><p>这里”根蒂”,”脐部”,”触感”三个属性均取得了最大的信息增益，因此可以任意挑选一个作为划分属性.类似的，对其它分支结点都可以进行上述操作, 最终得到的决策树如下图所示:<br><img src="/img/CompleteTree.png" style="height:400px;width:580px;"></p><h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h4><p>实际上，<strong>信息增益准则对取值较多的属性有所偏好</strong>, 比如, 我们把西瓜数据集的<strong>编号</strong>而当成一个属性, 将会产生17个分支，并且它对整个数据集 $D$ 的信息增益高达0.998.</p><p>为了减少这种偏好带来的不利影响, 可以使用<strong>C4.5</strong>算法进行改进. C4.5与ID3的主要区别在于C4.5使用”增益率”作为准则来选择最优划分属性, 而不再使用信息增益. 增益率的定义为:</p><script type="math/tex; mode=display">\text{Gain_ratio}(D,a)=\frac{\text{Gain}(D,a)}{\text{IV}(a)}</script><p>其中,</p><script type="math/tex; mode=display">\text{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}</script><p>称为属性 $a$ 的固有值. 一般来说, 属性 $a$ 的可能取值数目 $V$ 越大, 固有值通常也会越大.</p><p>与信息增益准则相反, <strong>增益率准则</strong> 对可取值数目较少的属性有所偏好，因此C4.5一般不会直接采用增益率最大的属性作为最优划分属性，而是选择一种启发式的方法: 先从所有候选划分属性中找出<strong>信息增益</strong>高于平均水平的属性，再从中选择<strong>增益率</strong>最高的.</p><h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>与ID3和C4.5不同，<strong>CART</strong>决策树算法使用”基尼指数”作为准则来选择划分属性.与信息熵类似，<strong>基尼值</strong> 也可以用来表述数据集 $D$ 的纯度:</p><script type="math/tex; mode=display">\text{Gini}(D)=\sum_{k=1}^K\sum_{k'\neq k}p_kp_{k'}=1-\sum_{k=1}^Kp_k^2</script><p>基尼指数反映了 $D$ 随机两个样本类别标记不一致的概率. 基尼指数越小，数据集 $D$ 的纯度越高. 有了基尼值之后, 我们可以给出<strong>基尼指数</strong>的定义:</p><script type="math/tex; mode=display">\text{Gini_index}(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Gini}(D)</script><p>接着从属性集 $A$ 中挑选出基尼指数最小的属性作为最优划分属性:</p><script type="math/tex; mode=display">a_*=\arg\max_{a \in A}\text{Gini_index}(D,a)</script><h3 id="决策树剪枝"><a href="#决策树剪枝" class="headerlink" title="决策树剪枝"></a>决策树剪枝</h3><p>在决策树学习的过程中，为了尽可能正确分类样本，结点划分将不断进行，这就可能造成决策树因分支过多而过拟合. 解决决策树过拟合的主要手段是剪枝，具体可以分为预剪枝和后剪枝两种不同的类型. 下面的表格将数据集 $D$ 划分为训练集(上)和测试集(下).</p><p><img src="/img/holdout.png" style="height:450px;width:530px;"></p><p>使用训练集生成了如下的如下的一棵决策树(每个结点打了编号方便说明):</p><p><img src="/img/CompleteTree2.png" style="height:330px;width:520px;"></p><h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p><strong>预剪枝</strong> 指的是在 <strong>决策树生成过程中</strong> , 对每个结点在划分前进行评估，如果当前结点的划分不能带来泛化性能的提升，则应该立即停止划分，并将当前结点标记为叶子结点.</p><p>使用信息增益准则，如果选取属性”脐部”来划分训练集，就会产生3个分支. 接着，评估是否应该进行这个划分.</p><ul><li>若进行划分, 图中的2, 3, 4号结点分别包含的训练样本(用多数类标记类别)为:<script type="math/tex; mode=display">\{1,2,3,14\}, \text{好瓜} \\\{6,7,15,17\}, \text{好瓜} \\\{10,16\}, \text{坏瓜}</script>测试集中只有编号为 ${4,5,8,11,12}$ 的样本被分类正确, 验证精度为 $\frac{5}{7}\times100\%=71.4\%$，因此应该用”脐部”进行划分. 后面的叶节点的划分也类似，见如下图中的分析.<br><img src="/img/precut.png" style="height:230px;width:520px;"></li></ul><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>与预剪枝相反, 后剪枝是先<strong>从训练集中生成一棵完整的决策树</strong> 然后 <strong>自底向上</strong> 对所有非叶子结点进行考察, 若将该结点对应的子树替换为叶结点能带来泛化性能的提升，则将子树替换为叶结点.</p><p>我们先将上一节中的决策树的6号结点替换为叶结点, 替换后的叶结点包含7和15两个训练样本，将该结点的类别标记为好瓜, 并可以计算出验证精度提高至 $57.1\%$, 于是后剪枝策略决定剪枝. 用同样的方法考察5,2,3,1号结点, 最后得到剪枝后的决策树如下所示:<br><img src="/img/CompleteTree3.png" style="height:300px;width:630px;"></p><h3 id="处理连续和缺失值"><a href="#处理连续和缺失值" class="headerlink" title="处理连续和缺失值"></a>处理连续和缺失值</h3><h4 id="连续值"><a href="#连续值" class="headerlink" title="连续值"></a>连续值</h4><p>前面讨论的都是基于离散属性生成决策树，如果一个属性是连续的，那么就应该离散化技术来处理连续属性，本节中要介绍的技术是二分法，这种方法正是C4.5算法中采用的机制.</p><p>同样给定数据集 $D$ 和连续属性 $a$, 如果 $a$ 在 $D$ 上出现了 $n$ 个不同的取值, 那么第一步就是将这 $n$ 个数按从小到大进行排序:</p><script type="math/tex; mode=display">\{a^1,a^2,...,a^n\}</script><p>然后是确定二分点, 二分点由排列后的集合中相邻两个数据点所确定:</p><script type="math/tex; mode=display">T_a=\big \{ \frac{a^i+a^{i+1}}{2}|1\leq i\leq n-1\big\}</script><p>也就是会产生一个包含有 $n-1$ 划分点的集合, 取任意划分点 $t \in T_a$, 那么该划分点可以将原始数据集 $D$ 划分为 $D^{-}_t$ 和  $D^{+}_t$ 左右两个部分 (分别表示小于 $t$ 和 大于 $t$ 的部分). 我们只需要取能够使得信息增益最大的划分点就可以了:</p><script type="math/tex; mode=display">\text{Gain}(D,a)=\max_{t \in T_a}\text{Gain}(D,a,t)=\max_{t \in T_a}[\text{Ent}(D)-\sum_{\lambda \in \{-,+\}}\frac{|D^{\lambda}_t|}{|D|}\text{Ent}(D_t^{\lambda})]</script><p>以下面的西瓜数据集为例:</p><p><img src="/img/watermelon2.png" style="height:450px;width:600px;"></p><p>属性”密度”是一个连续属性, 先将所有样本按”密度”列从小到大排好序, 然后计算出划分点的集合:</p><script type="math/tex; mode=display">T_{\text{密度}}=\{0.244, 0.294, 0.351, 0.381, 0.420, 0.459, 0.518, 0.574, 0.600, \\ 0.621, 0.636, 0.648, 0.661, 0.681, 0.708, 0.746\}</script><p>以划分点 $0.381$ 为例, 西瓜数据集 $D$ 被切分为:</p><script type="math/tex; mode=display">D = D^{-}_{0.381} \cup D^{+}_{0.381}</script><p>那么可以计算出:</p><script type="math/tex; mode=display">\begin{aligned}\text{Gain}(D,\text{密度},0.381)&=\text{Ent}(D)-\sum_{\lambda \in \{-,+\}}\frac{|D^{\lambda}_{0.381}|}{|D|}\text{Ent}(D_{0.381}^{\lambda}) \\&=\text{Ent}(D)-(\frac{|D^{+}_{0.381}|}{|D|}\text{Ent}(D_{0.381}^{+})+\frac{|D^{-}_{0.381}|}{|D|}\text{Ent}(D_{0.381}^{-})) \\&=0.262\end{aligned}</script><p>而实际上, 经过计算后:</p><script type="math/tex; mode=display">\begin{aligned}\text{Gain}(D,\text{密度})&=\max_{t \in T_a} \text{Gain}(D,\text{密度},t) \\&=\max\{\text{Gain}(D,\text{密度},0.244), \text{Gain}(D,\text{密度},0.294), \text{Gain}(D,\text{密度},0.351), ...\} \\&=\text{Gain}(D,\text{密度},0.381) \\&=0.262 \\\end{aligned}</script><p>所以属性”密度”的信息增益为 $0.262$ , 对应于划分点 $0.381$ . 同理, 另一个连续属性”含糖率”也可以采用同样的方法计算出其信息增益为 $0.349$ , 对应于划分点 $0.126$ . 其它属性仍然为离散属性，采用之前的方法计算即可, 最终选取”纹理”作为根节点划分属性, 并递归进行得到如下的一棵决策树.</p><p><img src="/img/CompleteTree4.png" style="height:250px;width:450px;"></p><h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4><p>有时候我们会碰到数据集中某些样本的某些属性值缺失的情况, 如果仅仅使用无缺失值的样本进行学习, 对数据信息是极大的浪费. 考虑如下的西瓜数据集:</p><p><img src="/img/watermelon3.png" style="height:450px;width:600px;"></p><p>可以看到仅有 ${4,7,14,16}$ 这4个样本的信息是完全的. 现在有如下两个问题待解决:</p><ul><li>如何在属性值缺失的情况下进行划分属性的选择?</li><li>给定划分属性, 若样本在该属性的值上缺失, 应该如何对样本进行划分?</li></ul><p>令 $\tilde{D}$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集. 问题1可以直接用 $\tilde{D}$ 来解决. 假设 $a$ 有 $V$ 个可取值</p><script type="math/tex; mode=display">\{a^1, a^2,..., a^V\}</script><p>并将 $\tilde{D}$ 划分为:</p><script type="math/tex; mode=display">\{\tilde{D}^1, \tilde{D}^2,...,\tilde{D}^V\}</script><p>有假设 $\tilde{D}$ 中的样本有 $K$ 个类别, 这 $K$ 个类别将 $\tilde{D}$ 划分为</p><script type="math/tex; mode=display">\{\tilde{D}_{1}, \tilde{D}_{2},...,\tilde{D}_{K}\}</script><p>很明显:</p><script type="math/tex; mode=display">\tilde{D}=\bigcup_{k=1}^K\tilde{D}_k=\bigcup_{v=1}^V\tilde{D}^v</script><p>对属性 $a$, 用 $\rho$ 表示无缺失值的样本比例, $\tilde{p}_k$ 表示无缺失值样本中第 $k$ 类所占的比例, $\tilde{r}_v$ 表示无缺失样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例. 那么:</p><script type="math/tex; mode=display">\rho = \frac{|\tilde{D}|}{|D|}</script><script type="math/tex; mode=display">\tilde{p}_k=\frac{|\tilde{D}_k|}{|\tilde{D}|}</script><script type="math/tex; mode=display">\tilde{r}_v=\frac{|\tilde{D}^v|}{|\tilde{D}|}</script><p>显然可知:</p><script type="math/tex; mode=display">\sum_{k=1}^K{\tilde{p}_k}=\sum_{v=1}^V{\tilde{r}_v}=1</script><p>据此, 我们将信息增益的计算式推广为:</p><script type="math/tex; mode=display">\begin{aligned}\text{Gain}(D,a)&=\rho\times \text{Gain}(\tilde{D},a) \\&=\rho\times(\text{Ent}(\tilde{D})-\sum_{v=1}^V\tilde{r}_v\text{Ent}(\tilde{D}^v))\end{aligned}</script><p>其中:</p><script type="math/tex; mode=display">\text{Ent}(\tilde{D})=-\sum_{k=1}^K\tilde{p}_k\log_2\tilde{p}_k</script><p>接下来我们要解决问题2:</p><ul><li>若样本 $x$ 在划分属性 $a$ 上的取值已知, 就直接划分进对应子节点即可, 样本权值相当于为 $1$;</li><li>若样本 $x$ 在划分属性 $a$ 上的取值未知, 则将 $x$ 同时划分进该划分属性的所有子节点, 且将样本的权值调整为 $\tilde{r}_v\times 1$, 这其实就是让同一个样本以不同的概率划分到不同的子节点中去.</li></ul><p>回到上面的西瓜数据集, 对于属性”色泽”而言, 无缺失值的样本为:</p><script type="math/tex; mode=display">\{2,3,4,6,7,8,9,10,11,12,14,15,16,17\}</script><p>其中正类和负类所占的比例各自为: $\tilde{p}_1=\frac{6}{14}$ 和 $\tilde{p}_2=\frac{8}{14}$, 由此可得出(色泽)无缺失数据集 $\tilde{D}$ 的信息熵为:</p><script type="math/tex; mode=display">\text{Ent}(\tilde{D})=-\sum_{k=1}^2\tilde{p}_k\log_2{\tilde{p}_k}=-(\frac{6}{14}\log_2\frac{6}{14}+\frac{8}{14}\log_2\frac{8}{14})=0.985</script><p>又令 $\tilde{D}^1$, $\tilde{D}^2$, $\tilde{D}^3$ 分别代表属性”色泽”上取值为”青绿”, “乌黑”以及”浅白”的无缺失值数据集的样本子集, 那么:</p><script type="math/tex; mode=display">\text{Ent}(\tilde{D}^1)=-(\frac{2}{4}\log_2\frac{2}{4}+\frac{2}{4}\log_2\frac{2}{4})=1.000 \\\text{Ent}(\tilde{D}^2)=-(\frac{4}{6}\log_2\frac{4}{6}+\frac{2}{6}\log_2\frac{2}{6})=0.918 \\\text{Ent}(\tilde{D}^3)=-(\frac{0}{4}\log_2\frac{0}{4}+\frac{4}{4}\log_2\frac{4}{4})=0.000 \\</script><p>所以无缺失样本子集 $\tilde{D}$ 上属性”色泽”的信息增益为:</p><script type="math/tex; mode=display">\begin{aligned}\text{Gain}(\tilde{D}, 色泽)&=\text{Ent}(\tilde{D})-\sum_{v=1}^V\tilde{r}_v\text{Ent}(\tilde{D}^v) \\&=0.985-(\frac{4}{14}\times1.000+\frac{6}{14}\times0.918+\frac{4}{14}\times0.000) \\&=0.306\end{aligned}</script><p>因此, 在整个样本集上 $D$ 上的信息增益为:</p><script type="math/tex; mode=display">\text{Gain}(D, 色泽)=\rho\times\text{Gain}(\tilde{D}, 色泽)=\frac{14}{17}\times0.306=0.252</script><p>同理, 其它属性也可以采用相同方法计算其信息增益, 最终得到属性”纹理”获得最大信息增益, 并划分出”清晰”,”稍糊”和”模糊”三个分支, 在”纹理”上取值明确的样本中:</p><script type="math/tex; mode=display">\text{清晰}:\{1,2,3,4,5,6,15\} \\\text{稍糊}:\{7,9,13,14,17\} \\\text{模糊}:\{11,12,16\}</script><p>而编号 $8$ 和 $10$ 在属性”纹理”上取值缺失, 以 $8$ 号样本为例, 它同时进入上面的三个子结点, 但权重需要分别调整为 $\frac{7}{15}$, $\frac{5}{15}$, $\frac{3}{15}$.</p><h3 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h3><p>前面我们介绍的决策树都属于单变量决策树, 单变量决策树的主要特点在于分类边界的每一段都是与坐标轴平行的, 有时候, 我们需要构建更加复杂的决策树, 那么就要用到多变量决策树了, 在这类决策树中, 非叶结点不再是针对某个属性，而是对属性的线性组合进行测试, 如下图所示, 这样构成的决策树具有的斜的划分边界.<br><img src="/img/multitree.png" style="height:200px;width:450px;"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机</title>
      <link href="/2019/11/11/ml-03/"/>
      <url>/2019/11/11/ml-03/</url>
      
        <content type="html"><![CDATA[<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><h4 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a><strong>间隔与支持向量</strong></h4><p>给定训练集</p><script type="math/tex; mode=display">D=\{x^{(1)}, x^{(2)}, ..., x^{(m)}\}, y^{(i)} \in \{-1, 1\}</script><p>现在使用支持向量机进行分类,其划分超平面的秒速如下:</p><script type="math/tex; mode=display">w^Tx+b=0</script><p>注意 $w$ 是平面的法向量, $b$则是位移. 根据点到平面的距离公式, 样本点 $x$ 到平面 $(w, b)$ 的距离为:</p><script type="math/tex; mode=display">r=\frac{|w^Tx+b|}{\left \| w\right \|}</script><p>因此, 如果能正确分类，若 $y^{(i)}=+1$, 则有 $w^Tx^{(i)} + b &gt; 0$, 若 $y^{(i)}=-1$, 则有 $w^Tx^{(i)} + b &lt; 0$, 令:</p><script type="math/tex; mode=display">\begin{equation}\begin{cases}w^Tx^{(i)} + b \geq +1,& y^{(i)} = +1\\w^Tx^{(i)} + b \leq -1,& y^{(i)} = -1\end{cases}\end{equation}</script><p>两个类别平面 $w^Tx^{(i)} + b = 1$ 和 $w^Tx^{(i)} + b = -1$上的点称为支持向量，两个异类支持向量到超平面的距离之和（也是两个类别平面的距离）称为间隔:</p><script type="math/tex; mode=display">\gamma = \frac{|b-1-(b + 1)|}{\left \| w\right \|}=\frac{2}{\left \| w\right \|}</script><p>现在我们的目标是找到合适的 $w, b$ 使得间隔最大化，也就是:</p><script type="math/tex; mode=display">\begin{aligned}&\max_{w, b}\frac{2}{\left \| w\right \|} \\&\text{s.t.}\;\;y^{(i)}(w^Tx^{(i)}+b) \geq 1, i=1,2,...,m\end{aligned}</script><p>上面的写法等价于:</p><script type="math/tex; mode=display">\begin{aligned}&\min_{w, b}\frac{1}{2}\left \| w\right \|^2 \\&\text{s.t.}\;\;y^{(i)}(w^Tx^{(i)}+b) \geq 1, i=1,2,...,m\end{aligned}</script><p>这就是支持向量机的<strong>基本型</strong>.</p><h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a><strong>拉格朗日乘子法</strong></h4><h4 id="1-等式约束"><a href="#1-等式约束" class="headerlink" title="1. 等式约束"></a>1. 等式约束</h4><p>考虑如下等式约束问题:</p><script type="math/tex; mode=display">\begin{aligned}&\min f(x) \\& \text{s.t.}\;\;g(x)=0\end{aligned}</script><p>也就是寻找某个合适 $x$ 的取值使得目标函数 $f(x)$ 最小并且满足 约束曲面 $g(x)=0$ 的约束.<br>首先, 我们要记住如下结论:</p><ul><li>曲面上某点的<strong>梯度的方向</strong>总是和经过该点的<strong>等高线(的切线)方向</strong>正交/垂直.</li><li>最优解一定存在于 $f(x)$ 与 $g(x)=0$ 的相切位置. (众所周知, 相离和相交一般不可能取得最优解)</li></ul><p>注意此处 $x$  为向量. 为了方便解释，我们假设它是一个二维向量. 由于 $g(x)=0$ 就是 $g(x)$ 的其中一条等高线, 依据上面的结论,</p><ul><li>在约束曲面上的某点 $x$ 处, $\nabla g(x)$ 正交于等高线 $g(x)=0$ 的切线.</li><li>由于最优解存在于 $f(x)$ 与 $g(x)=0$相切的位置, 因此最优解 $x$ 处 $f(x)$ 与 $g(x)=0$ 的切线是同一条, 那么 $\nabla f(x)$ 同样正交于 $g(x)=0$ 的切线.</li></ul><p>因此, $\nabla f(x)$ 和 $\nabla g(x)$ 必定同向或者反向, 也就是存在 $\lambda \neq 0$ 使得:</p><script type="math/tex; mode=display">\nabla f(x)+\lambda\nabla g(x)=0</script><p>这里 $\lambda$ 称为格朗日乘子, 定义拉格朗日函数：</p><script type="math/tex; mode=display">L(x, \lambda)=f(x)+\lambda g(x)</script><p>可以发现</p><script type="math/tex; mode=display">\nabla_xL(x,\lambda)=\nabla f(x)+\lambda\nabla g(x)=0</script><p>就是我们上面得到的式子, 同时,</p><script type="math/tex; mode=display">\nabla_{\lambda}L(x,\lambda)=g(x)=0</script><p>恰好是约束条件.这样一来约束优化问题就转化为了无约束优化问题.</p><h4 id="2-不等式约束"><a href="#2-不等式约束" class="headerlink" title="2. 不等式约束"></a>2. 不等式约束</h4><p>现在考虑不等式约束</p><script type="math/tex; mode=display">\begin{aligned}&\min f(x) \\& \text{s.t.}\;\;g(x)\leq 0\end{aligned}</script><ul><li>最优点落在 $g(x) &lt; 0$ 区域内, 那么不等式约束没有起到任何作用，相当于直接优化 $f(x)$, 相当于 $L(x, \lambda)$ 中取 $\lambda = 0$</li><li>最优点落在 $g(x) = 0$上, 变回了等式约束问题, 但此时 $\nabla f(x)$ 和 $\nabla g(x)$ 的方向必定相反, 即 $\lambda &gt; 0$.</li></ul><p>再简单理一下, 就是:</p><ul><li>$g(x) &lt; 0$ 时 $\lambda = 0$</li><li>$g(x) = 0$ 时 $\lambda &gt; 0$.</li></ul><p>那么原来的不等式约束优化可以转化为如下的优化问题:</p><script type="math/tex; mode=display">\begin{aligned}&L(x, \lambda)=f(x)+\lambda g(x) \\&\text{s.t.}\;\;\begin{cases}g(x) \leq 0; \\\lambda \geq 0; \\\lambda g(x)=0.\end{cases}\end{aligned}</script><p>上述优化问题的约束也称为KKT条件. 现在将KKT条件推广到 $m$个等式约束和 $n$ 个不等式约束上, 可以得到如下优化问题:</p><script type="math/tex; mode=display">\begin{aligned}&\min f(x) \\& \text{s.t.}\;\;h_i(x) = 0\;\;(i=1,2,...,m)\\& \;\;\;\;\;\;\;g_j(x)\leq 0\;\;(j=1,2,...,n)\end{aligned}</script><p>引入拉格朗日乘子 $\lambda = (\lambda_1, \lambda_2, …, \lambda_m)^T$ 和 $\mu = (\mu_1, \mu_2,…,\mu_n)^T$, 可以得到如下拉格朗日函数:</p><script type="math/tex; mode=display">L(x,\lambda,\mu)=f(x)+\sum_{i=1}^m\lambda_ih_i(x)+\sum_{j=1}^n\mu_jg_j(x)</script><p>由不等式约束引入的KKT条件( $j=1,2,…,n$ )为:</p><script type="math/tex; mode=display">\begin{aligned}\begin{cases}g_j(x) \leq 0; \\\mu_j \geq 0; \\\mu_jg_j(x)=0.\end{cases}\end{aligned}</script><p>显然, 在满足KKT的情况下:</p><script type="math/tex; mode=display">\sum_{i=1}^m\lambda_ih_i(x)+\sum_{j=1}^n\mu_jg_j(x) \leq 0</script><p>那么:</p><script type="math/tex; mode=display">L(x,\lambda,\mu) \leq f(x)+0=f(x)</script><p>反之, 如果KKT没有得到满足, 比如, 出现了 $h_i(x) \neq 0$ 或者 $g_i(x)&gt;0$, 就会可能出现：</p><script type="math/tex; mode=display">\sum_{i=1}^m\lambda_ih_i(x)+\sum_{j=1}^n\mu_jg_j(x) \rightarrow \infty</script><p>此时就会有:</p><script type="math/tex; mode=display">L(x,\lambda,\mu) \rightarrow \infty</script><p>据此，定义:</p><script type="math/tex; mode=display">\theta_p(x)=\max_{\lambda,\mu:\mu_i>0}L(x,\lambda,\mu)=\begin{cases}f(x) & \text{KKT is satisfised}; \\\infty &  \text{otherwise}. \\\end{cases}</script><p>所以我们可以发现, 在满足KKT的条件下:</p><script type="math/tex; mode=display">p=\min_x\theta_p(x)=\min_x\max_{\lambda,\mu:\mu_i>0}L(x,\lambda,\mu)=\min_xf(x)</script><p>也就是这种转换与我们原来求解的问题一致, 这一问题也成为<strong>主优化问题</strong>.与主优化问题相对应的，是它的对偶问题，我们首先定义:</p><script type="math/tex; mode=display">\theta_D(\lambda,\mu)=\min_xL(x,\lambda,\mu)</script><p>那么对偶优化问题为:</p><script type="math/tex; mode=display">d=\max_{\lambda,\mu:\mu_i>0}\theta_D(\lambda,\mu)=\max_{\lambda,\mu:\mu_i>0}\min_xL(x,\lambda,\mu)</script><p>一般情况下:</p><script type="math/tex; mode=display">d \leq p</script><p>但是如果$f,g$为凸函数,$h$为仿射函数时(具体来说,$f,g$的二阶导数/$\text{Hessian}$ 矩阵半正定, $h$在线性变换的基础上加了截距项)时, 有:</p><script type="math/tex; mode=display">d = p</script><p>那么,求解出对偶问题也就求解出了主问题.</p><h3 id="求解支持向量机"><a href="#求解支持向量机" class="headerlink" title="求解支持向量机"></a><strong>求解支持向量机</strong></h3><p>回顾支持向量机的基本型，它可以写成如下形式:</p><script type="math/tex; mode=display">\begin{aligned}&\min_{w, b}f(w,b)=\min_{w,b}\frac{1}{2}\left \| w\right \|^2 \\&\text{s.t.}\;\; g_i(w)=1-y^{(i)}(w^Tx^{(i)}+b) \leq 0, i=1,2,...,m\end{aligned}</script><p>其拉格朗日函数为:</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac{1}{2}\left \| w\right \|^2+\sum_{i=1}^m\alpha_i(1-y^{(i)}(w^Tx^{(i)}+b))</script><p>对应的对偶问题为:</p><script type="math/tex; mode=display">d=\max_{\alpha:\alpha_i>0}\min_{w,b}L(w,b,\alpha)</script><p>因此,先求 $L$ 对 $w$ 和 $b$ 的偏导数:</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\partial{L}}{\partial{w}}=w-\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}=\vec{0} \\&\frac{\partial{L}}{\partial{b}}=-\sum_{i=1}^m\alpha_iy^{(i)}=0\end{aligned}</script><blockquote><blockquote><p>矩阵求导术备注(注意 $a^Tb=b^Ta$):</p><script type="math/tex; mode=display">\begin{aligned}d\left\| w\right\|^2&=d(w^Tw) \\&=d(w^T)w+w^Tdw \\&=(dw)^Tw+w^Tdw \\&=w^Tdw+w^Tdw \\&=2w^Tdw \\&=(\frac{\partial{w^Tw}}{\partial{w}})^Tdw\end{aligned}</script></blockquote></blockquote><p>也就是:</p><script type="math/tex; mode=display">\begin{aligned}& w=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)} \\& 0=\sum_{i=1}^m\alpha_iy^{(i)}\end{aligned}</script><p>代入原来的拉格朗日函数，得到:</p><script type="math/tex; mode=display">\begin{aligned}L(w,b,\alpha)&=\frac{1}{2}\left \| w\right \|^2+\sum_{i=1}^m\alpha_i(1-y^{(i)}(w^Tx^{(i)}+b)) \\&=\frac{1}{2}w^Tw+\sum_{i=1}^m\alpha_i(1-y^{(i)}(w^Tx^{(i)}+b)) \\&=\frac{1}{2}(\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)})^T(\sum_{j=1}^m\alpha_jy^{(j)}x^{(j)})+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy^{(i)}(w^Tx^{(i)}+b) \\&=\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)}+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy^{(i)}w^Tx^{(i)}-\sum_{i=1}^m\alpha_iy^{(i)}b \\&=\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)}+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\alpha_iy^{(i)}\sum_{j=1}^m\alpha_jy^{(j)}x^{(j)T}x^{(i)}-0 \\&=\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)}+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(j)T}x^{(i)} \\&=\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)}+\sum_{i=1}^m\alpha_i-\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)} \\&=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)} \\\end{aligned}</script><p>因此，支持向量机的对偶问题为:</p><script type="math/tex; mode=display">\begin{aligned}& \max_{\alpha:\alpha_i>0} \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)}\\& \text{s.t.}\;\;\sum_{i=1}^m \alpha_iy^{(i)}=0, \\& \;\;\;\;\;\;\;\;\alpha_i \geq 0, \\& \;\;\;\;\;\;\;\;i=1,2,...,m\end{aligned}</script><p>此时 $\alpha$ 未知，但是如果求出了 $\alpha$, 就能代回原来的模型得到:</p><script type="math/tex; mode=display">f(x)=w^Tx+b=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)T}x+b</script><p>$x$ 代表当前输入模型中的样本. 上述过程的KKT条件为:</p><script type="math/tex; mode=display">\begin{aligned}\begin{cases}g_i(x) =1-y^{(i)}f(x^{(i)})\leq 0; \\\alpha_i \geq 0; \\\alpha_ig_i(x)=\alpha_i(1-y^{(i)}f(x^{(i)}))=0.\end{cases}\end{aligned}</script><p>注意最后一个约束: $\alpha_i(1-y^{(i)}f(x^{(i)}))=0$ :</p><ul><li>如果 $\alpha_i=0$，那么对应样本 $(x^{(i)},y^{(i)})$ 不会在求和中出现, 也就不会出现在模型 $f(x)$ 的求解中.</li><li>如果 $1-y^{(i)}f(x^{(i)})=0$, 也就是 $y^{(i)}(w^Tx^{(i)}+b)=1$, 那么样本 $(x^{(i)},y^{(i)})$ 出现在最大间隔边界(分类边界)上, 这个样本就是支持向量.</li></ul><p>因此，支持向量机的一个重要性质是: <strong>训练完成后, 大部分样本都不需要保留，最终模型只和支持向量有关.</strong></p><p>求解对偶问题, 可以采用SMO算法, 其基本思路是: 固定 $\alpha_i$ 之外的虽有参数，然后求 $\alpha_i$ 上的极值. 具体来说:<br>由于</p><script type="math/tex; mode=display">\sum_{i=1}^m\alpha_iy^{(i)}=0</script><p>随机选取一对变量 $\alpha_i$ 和 $\alpha_j$ (重复进行直至对偶问题的目标函数收敛，也就是尽可能增大), 固定其它参数, 得到:</p><script type="math/tex; mode=display">\alpha_iy^{(i)}+\alpha_jy^{(j)}=-\sum_{k\neq i,j}\alpha_ky^{(k)}=c \;(c\text{看成常数})</script><p>所以</p><script type="math/tex; mode=display">\alpha_j=\frac{c-\alpha_iy^{(i)}}{y^{(j)}}</script><p>将其代回原来的对偶问题后会发现变成了一个只关于 $\alpha_i$ 的单变量二次规划问题. 且仅有的约束是 $\alpha_i \geq 0$.<br>给定支持向量 $(x^{(s)}, y^{(s)})$, 并定义支持向量的下标集:</p><script type="math/tex; mode=display">S = \{i|\alpha_i>0,i=1,2,...,.\}</script><p>我们可以得到:</p><script type="math/tex; mode=display">y^{(s)}(\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)T}x^{(s)}+b)=1\Longleftrightarrow y^{(s)}(\sum_{i \in S}\alpha_iy^{(i)}x^{(i)T}x^{(s)}+b)=1</script><p>这是因为非支持向量的 $\alpha_i=0$, 因此可以较少的计算量得到:</p><script type="math/tex; mode=display">b=\frac{1}{y^{(s)}}-\sum_{i \in S}\alpha_iy^{(i)}x^{(i)T}x^{(s)}</script><p>这里的输入样本只用到了任意一个支持向量 $(x^{(s)}, y^{(s)})$, 为了提高鲁棒性，可以采用多个支持向量并计算平均值:</p><script type="math/tex; mode=display">b=\frac{1}{|S|}\sum_{s \in S}(\frac{1}{y^{(s)}}-\sum_{i \in S}\alpha_iy^{(i)}x^{(i)T}x^{(s)})</script><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a><strong>核函数</strong></h3><p>由于原始样本空间的数据不一定线性可分（例如”异或”问题）, 这时候就需要将原始样本 $x$ 映射到高维空间，得到 $\phi(x)$，这时候特征空间中划分超平面所对应的模型为:</p><script type="math/tex; mode=display">f(x)=w^T\phi(x)+b</script><p>我们求解的优化问题变成了:</p><script type="math/tex; mode=display">\begin{aligned}& \min_{w,b}\frac{1}{2}\left\|w\right\|^2 \\& \text{s.t.}\;y^{(i)}(w^T\phi(x^{(i)})+b)\geq1,i=1,2,...,m\end{aligned}</script><p>其对偶优化问题为:</p><script type="math/tex; mode=display">\begin{aligned}& \max_{\alpha:\alpha_i>0} \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}\phi(x^{(i)})^T\phi(x^{(j)})\\& \text{s.t.}\;\;\sum_{i=1}^m \alpha_iy^{(i)}=0, \\& \;\;\;\;\;\;\;\;\alpha_i \geq 0, \\& \;\;\;\;\;\;\;\;i=1,2,...,m\end{aligned}</script><p>通常情况下, $\phi(x^{(i)})^T\phi(x^{(j)})$ 的计算代价比较大，因为所映射的高维空间的维数比较高，甚至可能是无穷维，因此，我们可以定义如下的一个函数（称为核函数）:</p><script type="math/tex; mode=display">\kappa(x^{(i)},x^{(j)})=<\phi(x^{(i)}), \phi(x^{(j)})>=\phi(x^{(i)})^T\phi(x^{(j)})</script><p>那么对偶问题可以重写为:</p><script type="math/tex; mode=display">\begin{aligned}& \max_{\alpha:\alpha_i>0} \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}\kappa(x^{(i)},x^{(j)})\\& \text{s.t.}\;\;\sum_{i=1}^m \alpha_iy^{(i)}=0, \\& \;\;\;\;\;\;\;\;\alpha_i \geq 0, \\& \;\;\;\;\;\;\;\;i=1,2,...,m\end{aligned}</script><p>求解出来的结果为:</p><script type="math/tex; mode=display">\begin{aligned}f(x)&=w^T\phi(x)+b \\&=\sum_{i=1}^m\alpha_iy^{(i)}\phi(x^{(i)})^T\phi(x)+b \\&=\sum_{i=1}^m\alpha_iy^{(i)}\kappa(x^{(i)},x)+b\end{aligned}</script><p>也就是模型最优解可以通过训练样本的核函数展开,这个展式称为”支持向量展式”。那么应该如何寻找核函数 $\kappa(·,·)$ 呢, 这里需要记住一个小结论: <strong>核函数所对应的核矩阵 $K=[\kappa(x^{(i)},x^{(j)})],i,j=1,…,m$ 总是对称且半正定的.</strong> 下面是几种常见的核函数:</p><script type="math/tex; mode=display">\text{线性核:}\;\kappa(x^{(i)},x^{(j)})=x^{(i)T}x^{(j)} \\\text{多项式核:}\;\kappa(x^{(i)},x^{(j)})=(x^{(i)T}x^{(j)})^d \\\text{高斯核:}\;\kappa(x^{(i)},x^{(j)})=\exp(-\frac{\left\|x^{(i)}-x^{(j)}\right\|^2}{2\sigma^2})\;(\sigma>0为高斯核的带宽) \\</script><h3 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a><strong>软间隔与正则化</strong></h3><p>为了防止缓解支持向量机的过拟合问题, 应该允许它在一些样本上出错, 由此引入了“软间隔”的概念. 而前面讲的都是”硬间隔”, “软间隔”在设计上满足下面两点:</p><ul><li>允许某些样本不满足: $y^{(i)}(w^Tx^{(i)}+b)\geq1$</li><li>在最大化间隔的同时, 不满足约束的样本应该尽可能少.</li></ul><p>据此，可以将原来的优化目标重写为:</p><script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^ml_{0/1}(y^{(i)}(w^Tx^{(i)}+b)-1)</script><p>此处 $l_{0/1}$ 是一个”0/1损失函数”:</p><script type="math/tex; mode=display">\begin{aligned}l_{0/1}(z)=\begin{cases}1, & \text{if}\;z < 0; \\0, & \text{otherwise.}\end{cases}\end{aligned}</script><p>当约束被违背 $y^{(i)}(w^Tx^{(i)}+b) &lt; 1$, 目标就该是计算损失, 但是0/1损失函数不连续, 我们更喜欢用hinge损失来替代, hinge损失函数是凸且连续的:</p><script type="math/tex; mode=display">l_{hinge}(z)=\max(0,1-z)</script><p>那么优化目标又变成了:</p><script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^m\max(0,1-y^{(i)}(w^Tx^{(i)}+b))</script><p>引入松弛变量 $\xi_i\geq0$, 上式可以重写为:</p><script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^m\xi_i</script><p>由于允许一部分样本出错，原来的两个分类边界可以做出一定的退让:</p><script type="math/tex; mode=display">y^{(i)}(w^Tx^{(i)}+b)=1-\xi_i \\y^{(i)}(w^Tx^{(i)}+b)=\xi_i-1</script><p>于是完整的优化问题变成了:</p><script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^m\xi_i \\\text{s.t.}\;y^{(i)}(w^Tx^{(i)}+b)\geq1-\xi_i \\\;\;\xi_i\geq0,i=1,2,...,m</script><p>因此该问题的拉格朗日函数为:</p><script type="math/tex; mode=display">L(w,b,\xi,\alpha,\mu)=\frac{1}{2}\left\|w\right\|^2+C\sum_{i=1}^m\xi_i+\sum_{i=1}^m\alpha_i(1-\xi_i-y^{(i)}(w^Tx^{(i)}+b))-\sum_{i=1}^m\mu_i\xi_i</script><p>求 $L$ 对 $w,b,\xi_i$ 的偏导数:</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\partial{L}}{\partial{w}}=w-\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}=\vec{0} \\&\frac{\partial{L}}{\partial{b}}=-\sum_{i=1}^m\alpha_iy^{(i)}=0 \\&\frac{\partial{L}}{\partial{\xi_i}}=C-\alpha_i-\mu_i=0\end{aligned}</script><p>求得:</p><script type="math/tex; mode=display">\begin{aligned}&w=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)} \\&0=\sum_{i=1}^m\alpha_iy^{(i)} \\&C=\alpha_i+\mu_i\end{aligned}</script><p>显然 $0\leq\alpha_i=C-\mu_i \leq C + 0 = C$, 上面几个式子代入回格朗日函数得到：</p><script type="math/tex; mode=display">\begin{aligned}& \max_{\alpha:\alpha_i>0} \sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}x^{(i)T}x^{(j)}\\& \text{s.t.}\;\;\sum_{i=1}^m \alpha_iy^{(i)}=0, \\& \;\;\;\;\;\;\;\;0\leq \alpha_i \leq C, \\& \;\;\;\;\;\;\;\;i=1,2,...,m\end{aligned}</script><p>同样地，软间隔支持向量机的KKT条件为:</p><script type="math/tex; mode=display">\begin{aligned}\begin{cases}y^{(i)}f(x^{(i)}) \geq 1-\xi_i, \\\xi_i \geq 0,\\\alpha_i\geq0, \\\alpha_i(1-\xi_i-y^{(i)}f(x^{(i)}))=0,\\\mu_i\geq0, \\\mu_i\xi_i=0\end{cases}\end{aligned}</script><ul><li>$\alpha_i=0$, 对应样本对 $f(x)$ 求解没有影响;</li><li>$\alpha_i &gt; 0$, $y^{(i)}f(x^{(i)})=1-\xi_i$, 样本为支持向量, 落在分类边界上(但不一定是最大间隔边界，因为引入了松弛变量)</li><li>$\alpha_i &lt; C$, 则 $\mu_i &gt; 0$, 那么必定有 $\xi_i = 0$, 所以样本就恰好落在最大间隔边界上</li><li>$\alpha_i=C$, 则 $\mu_i=0$, 若 $0&lt;\xi_i\leq1$, 那么样本落在最大间隔边界内部, 若 $\xi_i &gt; 1$, 样本分类错误.</li></ul><p>最后, 简单提一提支持向量回归SVR, 传统回归模型通常基于模型输出 $f(x)$ 和 真实标签 $y$ 之间的差别来计算损失, 而SVR通常则是容忍 $f(x)$ 与 $y$ 之间有 $\epsilon$ 的偏差, 当 $|f(x)-y|&gt;\epsilon$ 时才计算损失, 这就构建了一个宽度为 $2\epsilon$ 的间隔带, 若训练样本落入间隔带中，则认为预测时正确的. </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成模型</title>
      <link href="/2019/11/04/ml-02/"/>
      <url>/2019/11/04/ml-02/</url>
      
        <content type="html"><![CDATA[<h3 id="生成学习"><a href="#生成学习" class="headerlink" title="生成学习"></a>生成学习</h3><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><p>线性回归, 逻辑回归和softmax回归都是判别式学习算法，也就是直接学习 $p(y|x;\theta)$, 生成学习的思路则不同，以分类问题为例，它分别对<strong>不同类别</strong>的<strong>特征分布</strong>进行建模，比如二分类，需要分别对 $p(x|y=0)$ 和 $p(x|y=1)$ 建模，更具体的说，生成学习算法需要对 $p(x|y)$ 和 $p(y)$ 建模.<br>接着运用贝叶斯定理, 得到:</p><script type="math/tex; mode=display">p(y|x)=\frac{p(x|y)p(y)}{p(x)}</script><p>各个术语说明如下:</p><ul><li><p>$p(y)$, 先验概率, “原因”的概率.</p></li><li><p>$p(x|y)$, 似然函数, $L(y|x)=C\cdot p(x|y)$, 产生<strong>数据</strong>$x$的可能称为似然, $y$就像是参数一样.</p></li><li><p>$p(y|x)$, 后验概率, 贝叶斯定理求的是结果到原因的概率，这里相当于在知道观测<strong>结果</strong>的条件下得到<strong>原因</strong>的概率.</p></li><li><p>$p(x)$, 证据因子, 用全概率公式求解, $p(x)=p(x|y=0)p(y=0)+p(x|y=1)p(y=1)$.</p></li></ul><p>据此, 我们知道最大后验概率估计的方法为:</p><script type="math/tex; mode=display">\arg\max_{y}p(y|x)=\arg\max_{y}\frac{p(x|y)p(y)}{p(x)} = \arg\max_{y} p(x|y)p(y)</script><p>此处由于我们求的最大值只跟 $y$ 有关, 相当于 $x$ 就是一个常量了, 可以直接把分母移除, 它对最后求出来的最大值没有任何影响. 与判别式模型不同, 我们不是最大化 $p(y|x)$, 而是最大化 $p(x|y)p(y)$ 也就是 $p(x,y)$ 这一联合概率. <strong>牢牢记住判别模型需要对 $p(x|y)$ 和 $p(y)$ 建模</strong>, 举个例子, 高斯判别分析 (GDA) 就是一个生成学习算法, 如果用它来做二分类, 那么可以进行如下建模:</p><script type="math/tex; mode=display">y\sim Bernoulli(\phi)</script><script type="math/tex; mode=display">x|y=0\sim\mathcal N(\mu_0, \Sigma)</script><script type="math/tex; mode=display">x|y=1\sim\mathcal N(\mu_1, \Sigma)</script><p>也就是：</p><script type="math/tex; mode=display">p(y)=\phi^y(1-\phi)^{1-y}</script><script type="math/tex; mode=display">p(x|y=0)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))</script><script type="math/tex; mode=display">p(x|y=1)=\frac{1}{(2\pi^{\frac{n}{2}})|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))</script><p>接下来的计算方法就是极大似然估计了:</p><script type="math/tex; mode=display">\begin{aligned}l(\phi,\mu_0,\mu_1,\Sigma)&=\log\prod_{i=1}^mp(x^{(i)}, y^{(i)};\phi, \mu_0, \mu_1, \Sigma) \\&=\sum_{i=1}^m\log{p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)}\end{aligned}</script><h4 id="高斯判别分析和逻辑回归的关系"><a href="#高斯判别分析和逻辑回归的关系" class="headerlink" title="高斯判别分析和逻辑回归的关系"></a>高斯判别分析和逻辑回归的关系</h4><p>实际上, GDA和Logistic回归存在如下关系:</p><script type="math/tex; mode=display">p(y|x;\mu_0,\mu_1,\Sigma,\phi)=\frac{1}{1+\exp(-\theta^Tx)}</script><p>其中, $\theta$ 可以表示为 $\mu_0, \mu_1, \Sigma, \phi$的某种函数, 一般来说:</p><ul><li>GDA的假设比逻辑回归要强, 也就是如果特别肯定 $p(x|y)$ 是一个多变量的高斯分布，那么使用GDA的建模效果比逻辑回归要好, 特别在大规模的数据集上效果更为明显;</li><li>逻辑回归的假设要弱一些, 但这也意味着它的鲁棒性和容错性强一些, 在多数情况下, 我们并无法确定 $x|y$ 会服从什么样的一个分布, 那么使用逻辑回归来解决会更好, 比方说 $x|y$ 服从泊松分布, 那么使用GDA进行建模肯定是比较差的，但是逻辑回归却很合适.</li></ul><h4 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h4><p>现在我们要建立一个用于垃圾邮件分类的模型, 这时候我们就要用到朴素贝叶斯了, 朴素贝叶斯也是一种生成学习算法, 按照前面提到的内容, 生成学习需要对 $p(x|y)$ 和 $p(y)$ 建模, 这里很明显:</p><script type="math/tex; mode=display">y\sim Bernoulli(\phi_y)</script><p>也就是:</p><script type="math/tex; mode=display">p(y)=\phi_y^y(1-\phi_y)^{1-y}</script><p>因为垃圾邮件和非垃圾邮件可以分别用0和1来表示, 那 $x|y$ 服从的分布呢? 由于 $x$ 常常表示为独热编码, 因此 $x$ 中的每一个分量$x_j, j=1,…,n$ 的取值通常也是非0即1，代表这个位置上的单词是否被取到, 那么可以假设:</p><script type="math/tex; mode=display">x_j|y=0\sim Bernoulli(\phi_{j|y=0})</script><script type="math/tex; mode=display">x_j|y=1\sim Bernoulli(\phi_{j|y=1})</script><p>那么可以得到:</p><script type="math/tex; mode=display">p(x_j|y=0)=\phi_{j|y=0}^{x_j}(1-\phi_{j|y=0})^{1-x_j}</script><script type="math/tex; mode=display">p(x_j|y=1)=\phi_{j|y=1}^{x_j}(1-\phi_{j|y=1})^{1-x_j}</script><p>根据马尔可夫性质:</p><script type="math/tex; mode=display">p(x|y=0)=\prod_{j=1}^np(x_j|y=0)=\prod_{j=1}^n \phi_{j|y=0}^{x_j}(1-\phi_{j|y=0})^{1-x_j}</script><script type="math/tex; mode=display">p(x|y=1)=\prod_{j=1}^np(x_j|y=1)=\prod_{j=1}^n \phi_{j|y=1}^{x_j}(1-\phi_{j|y=1})^{1-x_j}</script><p>所以</p><script type="math/tex; mode=display">\begin{aligned}l(\phi_{j|y=0}, \phi_{j|y=1}, \phi_y)&=\log\prod_{i=1}^mp(x^{(i)},y^{(i)}) \\&=\sum_{i=1}^m\log p(x^{(i)},y^{(i)}) \\&=\sum_{i=1}^m\log p(x^{(i)}|y^{(i)})p(y^{(i)}) \\&=\sum_{i=1}^m\log[\prod_{j=1}^np(x_j^{(i)}|y^{(i)})]p(y^{(i)}) \\&=\sum_{i=1}^m(\log\prod_{j=1}^np(x_j^{(i)}|y^{(i)})+\log p(y^{(i)})) \\&=\sum_{i=1}^m(\sum_{j=1}^n\log p(x_j^{(i)}|y^{(i)})+\log p(y^{(i)})) \\&=\sum_{i=1}^m(\sum_{j=1}^n\log[\phi_{j|y}^{x^{(i)}_j}(1-\phi_{j|y})^{1-x^{(i)}_j}]+\log[\phi_y^{y^{(i)}}(1-\phi_y)^{1-y^{(i)}}]) \\&=\sum_{i=1}^m\sum_{j=1}^n[x^{(i)}_j\log\phi_{j|y}+(1-x^{(i)}_j)\log(1-\phi_{j|y})]+\sum_{i=1}^m[y^{(i)}\log\phi_y+(1-y^{(i)})\\&\log(1-\phi_y)] \\&=l_1^{'}+l_2^{'}\end{aligned}</script><p>似然函数对参数 $\phi_{j|y=0}$ 求偏导数:</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial{l}}{\partial{\phi_{j|y=0}}} &= \frac{\partial{l_1^{'}}}{\partial{\phi_{j|y=0}}} \\&=\frac{\partial{(\sum_{i=1}^m\sum_{j=1}^n[x^{(i)}_j\log\phi_{j|y}+(1-x^{(i)}_j)\log(1-\phi_{j|y})])}}{\partial{\phi_{j|y=0}}}\\&=\frac{\partial{(\sum_{i=1}^m\sum_{j=1}^n[x^{(i)}_j\log\phi_{j|y=0}\{y^{(i)}=0\}+(1-x^{(i)}_j)\log(1-\phi_{j|y=0})\{y^{(i)}=0\}])}}{\partial{\phi_{j|y=0}}} \\&=\sum_{i=1}^m[\frac{x^{(i)}_j}{\phi_{j|y=0}}\{y^{(i)}=0\}-\frac{1-x^{(i)}_j}{1-\phi_{j|y=0}}\{y^{(i)}=0\}] \\&=\sum_{i=1}^m\frac{x^{(i)}_j-x^{(i)}_j\phi_{j|y=0}-\phi_{j|y=0}+x^{(i)}_j\phi_{j|y=0}}{\phi_{j|y=0}(1-\phi_{j|y=0})}\{y^{(i)}=0\} \\&=\sum_{i=1}^m\frac{x^{(i)}_j-\phi_{j|y=0}}{\phi_{j|y=0}(1-\phi_{j|y=0})}\{y^{(i)}=0\} \\&= 0\end{aligned}</script><p>也即是：</p><script type="math/tex; mode=display">\begin{aligned}\sum_{i=1}^m(x^{(i)}_j-\phi_{j|y=0})\{y^{(i)}=0\}=0 \Longleftrightarrow\sum_{i=1}^mx^{(i)}_j\{y^{(i)}=0\}-\sum_{i=1}^m\phi_{j|y=0}\{y^{(i)}=0\}=0\end{aligned}</script><p>所以:</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=0}&=\frac{\sum_{i=1}^mx^{(i)}_j\{y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}} \\&=\frac{\sum_{i=1}^m1\{x^{(i)}_j=1\}\cdot1\{y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}} \\&=\frac{\sum_{i=1}^m1\{(x^{(i)}_j=1)\;\text{and}\;(y^{(i)}=0)\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}\end{aligned}</script><p>同理对参数 $\phi_{j|y=1}$ 可以得到:</p><script type="math/tex; mode=display">\phi_{j|y=1}=\frac{\sum_{i=1}^m1\{(x^{(i)}_j=1)\;\text{and}\;(y^{(i)}=1)\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}</script><p>最后对参数 $\phi_y$ 求偏导:</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial{l}}{\partial{\phi_y}} &= \frac{\partial{l_2^{'}}}{\partial{\phi_y}} \\&=\frac{\partial{(\sum_{i=1}^m[y^{(i)}\log\phi_y+(1-y^{(i)})\log(1-\phi_y)])}}{\partial{\phi_y}} \\&=\sum_{i=1}^m[\frac{y^{(i)}}{\phi_y}-\frac{1-y^{(i)}}{1-\phi_y}] \\&=\sum_{i=1}^m[\frac{y^{(i)}-y^{(i)}\phi_y-\phi_y+y^{(i)}\phi_y}{\phi_y(1-\phi_y)}] \\&=\sum_{i=1}^m[\frac{y^{(i)}-\phi_y}{\phi_y(1-\phi_y)}] \\&=0\end{aligned}</script><p>也即是:</p><script type="math/tex; mode=display">\sum_{i=1}^m(y^{(i)}-\phi_y)=0 \Longleftrightarrow \sum_{i=1}^my^{(i)}-m\phi_y=0</script><p>所以(注意观察，其实就是频率):</p><script type="math/tex; mode=display">\phi_y=\frac{\sum_{i=1}^my^{(i)}}{m}=\frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}</script><p>最后，如果来了一个新样本的特征向量 $x$, 直接采用贝叶斯公式和全概率公式计算就可以了:</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x)&=\frac{p(x|y=1)p(y=1)}{p(x)} \\&=\frac{p(x|y=1)p(y=1)}{\sum_yp(x,y)} \\&=\frac{p(x|y=1)p(y=1)}{\sum_yp(x|y)p(y)} \\&=\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y)+p(x|y=0)p(y=0)} \\&=\frac{(\prod_{j=1}^np(x_j|y=1))p(y=1)}{(\prod_{j=1}^np(x_j|y=1))p(y)+(\prod_{j=1}^np(x_j|y=0))p(y=0)}\end{aligned}</script><p>特别要注意的是, 如果 $x_j$ 不是二值化的，而是有$k, k \geq 2$个取值, 那么我们只要把 $p(x_j|y)$的建模从二项分布改成多项式分布, 对于连续型的取值, 也可以通过区间划分的方法转换为小规模的离散值的集合再应用多项式分布.</p><h4 id="拉普斯平滑"><a href="#拉普斯平滑" class="headerlink" title="拉普斯平滑"></a>拉普斯平滑</h4><p>如果字典中的第 $j$ 个单词 $x_j$ 从来没有在训练集的样本中出现过, 那么通过训练集训练得到的参数:</p><script type="math/tex; mode=display">1\{x_j = 1\} = 0 \Longrightarrow \phi_{j|y} = 0</script><p>这时候如果了来了一封包含单词 $x<em>j$ 的邮件，那么模型调用训练得到的参数 $\phi</em>{j|y}$ 进行预测:</p><script type="math/tex; mode=display">\phi_{j|y} = 0\Longrightarrow p(x_j|y)=0 \Longrightarrow p(x|y)=0</script><p>从而得到:</p><script type="math/tex; mode=display">\begin{aligned}p(y|x)&=\frac{p(x|y)p(y)}{p(x)}=\frac{p(x|y)p(y)}{\sum_yp(x,y)}=\frac{p(x|y)p(y)}{\sum_yp(x|y)p(y)}=\frac{0}{0}\end{aligned}</script><p>这样的结果不是我们想要的，如果给定$m$个观测样本：</p><script type="math/tex; mode=display">\{x_1,x_2,...,x_m\}</script><p>定义如下有 $k$ 个取值的多项式分布:</p><script type="math/tex; mode=display">p(x_i=j)=\phi_j, j \in \{1, 2, ..., k\}</script><p>一般来是，其极大似然估计的结果为（注意观察，其实是频率）:</p><script type="math/tex; mode=display">\phi_j=\frac{\sum_{i=1}^m1\{x_i=j\}}{m}</script><p>为了应对 $\phi_j$ 可能出现取值为0的情况，可以引入拉普拉斯平滑:</p><script type="math/tex; mode=display">\phi_j=\frac{\sum_{i=1}^m1\{x_i=j\} + 1}{m + k}</script><p>注意 $\sum_{i=1}^m\phi_j=1$依然是成立的.回到垃圾邮件分类问题上来, 各个参数的求解结果根据拉普拉斯平滑修改为:</p><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=0}=\frac{\sum_{i=1}^m1\{(x^{(i)}_j=1)\;\text{and}\;(y^{(i)}=0)\}+1}{\sum_{i=1}^m1\{y^{(i)}=0\}+2}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\phi_{j|y=1}=\frac{\sum_{i=1}^m1\{(x^{(i)}_j=1)\;\text{and}\;(y^{(i)}=1)\}+1}{\sum_{i=1}^m1\{y^{(i)}=1\}+2}\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型</title>
      <link href="/2019/10/31/ml-01/"/>
      <url>/2019/10/31/ml-01/</url>
      
        <content type="html"><![CDATA[<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h4><ul><li>模型假设:<script type="math/tex; mode=display">h_{\theta}(x)=\sum_{i=0}^n\theta_ix_i=\theta^Tx</script></li><li>目标函数:<script type="math/tex; mode=display">J(\theta)=\frac{1}{2}\sum^{m}_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^2</script></li><li>求单个样本的梯度：<script type="math/tex; mode=display">\frac{\partial}{\theta_j}J(\theta)=(h_{\theta}(x)-y)x_j</script></li><li>随机梯度下降：<script type="math/tex; mode=display">\theta_j = \theta_j - \alpha(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j</script></li><li>批量梯度下降：<script type="math/tex; mode=display">\theta_j = \theta_j - \alpha\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}_j</script></li></ul><h4 id="正规方程的推导"><a href="#正规方程的推导" class="headerlink" title="正规方程的推导"></a>正规方程的推导</h4><p>由于 $z^Tz=\sum_iz_i^2$,</p><p>所以</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{2}\sum^{m}_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)</script><p>根据$dJ=(\frac{\partial{J}}{\partial{\theta}})^Td\theta$,</p><p>所以</p><script type="math/tex; mode=display">\begin{aligned}dJ&=\frac{1}{2}d(X\theta-y)^T(X\theta-y)) \\&=\frac{1}{2}(d(X\theta-y)^T)(X\theta-y)+(X\theta-y)^Td(X\theta-y)) \\&=\frac{1}{2}([d(X\theta-y)]^T(X\theta-y)+(X\theta-y)^Td(X\theta-y)) \\&=\frac{1}{2}([X\cdot d\theta]^T(X\theta-y)+(X\theta-y)^TX\cdot d\theta) \\&=\frac{1}{2}((X\theta-y)^TX\cdot d\theta+(X\theta-y)^TX\cdot d\theta) \\&=(X\theta-y)^TX\cdot d\theta\end{aligned}</script><p>因此 $(\frac{\partial{J}}{\partial{\theta}})^T=(X\theta-y)^TX$</p><p>令 $\frac{\partial{J}}{\partial{\theta}}=X^T(X\theta-y)=0$</p><p>得出最优解为: $\theta=(X^TX)^{-1}X^Ty$</p><p>关于矩阵/向量求导法则请参考: <a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24709748</a></p><h4 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h4><p>首先, 我们知道</p><script type="math/tex; mode=display">y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}</script><p>噪声 $\epsilon^{(i)}$ 是独立同分布的, 且满足 $\epsilon^{(i)}\sim\mathcal N(0, \sigma^2)$, 写成数学公式:</p><script type="math/tex; mode=display">p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})</script><p>又因为 $\epsilon^{(i)}=y^{(i)}-\theta^Tx^{(i)}$, 那么$y^{(i)}|x^{(i)};\theta\sim\mathcal N(0, \sigma^2)$, 写成数学形式为:</p><script type="math/tex; mode=display">p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})</script><p>给出似然函数并最大化该函数:</p><script type="math/tex; mode=display">L(\theta)=L(\theta;X, y)=p(y|X;\theta)</script><p>也就是</p><script type="math/tex; mode=display">\begin{aligned}L(\theta)&=\prod^m_{i=1}p(y^{(i)}|x^{(i)};\theta) \\&=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\\end{aligned}</script><p>取似然函数的对数:</p><script type="math/tex; mode=display">\begin{aligned}l(\theta)&=\log L(\theta) \\&=\log\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\&=\sum_{i=1}^m\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\&=\sum_{i=1}^m[\log\frac{1}{\sqrt{2\pi}\sigma}+\log\exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})] \\&=m\log\frac{1}{\sqrt{2\pi}\sigma}+\sum_{i=1}^m[-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}] \\&=m\log\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}\cdot\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2 \\&=m\log\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2}\cdot J(\theta)\end{aligned} \\</script><p>所以最大化似然函数 $l(\theta)$ 等价于最小化目标函数 $J(\theta)$, 并且我们还发现了参数 $\theta$ 的选择并不依赖于方差 $\sigma^2$.</p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><h4 id="基本知识-1"><a href="#基本知识-1" class="headerlink" title="基本知识"></a>基本知识</h4><ul><li><p>sigmoid函数:</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{(-z)}}</script></li><li><p>sigmoid函数求导</p><script type="math/tex; mode=display">g'(z)=g(z)(1-g(z))</script></li><li><p>模型假设</p><script type="math/tex; mode=display">h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script></li><li><p>目标函数</p><script type="math/tex; mode=display">J(\theta)=-\sum_{i=1}^m(y^{(i)}\log{[h_{\theta}(x^{(i)})]}+(1-y^{(i)})\log{[1-h_{\theta}(x^{(i)})]})</script></li></ul><h4 id="概率解释-1"><a href="#概率解释-1" class="headerlink" title="概率解释"></a>概率解释</h4><p>不同于线性回归，逻辑回归主要是用来解决分类问题，可以做出如下假设:</p><script type="math/tex; mode=display">P(y=1|x;\theta)=h_{\theta}(x)</script><script type="math/tex; mode=display">P(y=0|x;\theta)=1-h_{\theta}(x)</script><p>或者换如下简洁的写法:</p><script type="math/tex; mode=display">P(y|x;\theta)=[h_{\theta}(x)]^y[1-h_{\theta}(x)]^{(1-y)}</script><p>接着，采用极大似然估计法最大化：</p><script type="math/tex; mode=display">\begin{aligned}L(\theta)&=L(\theta; X, y)=p(y|X; \theta) \\&=\prod^m_{i=1}p(y^{(i)}|x^{(i)};\theta) \\&=\prod^m_{i=1}[h_{\theta}(x^{(i)})]^{y^{(i)}}[1-h_{\theta}(x^{(i)})]^{(1-y^{(i)})}\end{aligned}</script><p>所以</p><script type="math/tex; mode=display">\begin{aligned}l(\theta)&=\log{L(\theta)} \\&=\sum_{i=1}^m(y^{(i)}\log{[h_{\theta}(x^{(i)})]}+(1-y^{(i)})\log{[1-h_{\theta}(x^{(i)})]}) \\&=-J(\theta)\end{aligned}</script><p>极大似然函数对模型参数求偏导数（可以采用一个样本进行计算再求和，省去很多上下标计算）:</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial{\theta_j}}l(\theta)&=\sum_{i=1}^m[y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}g'(\theta^Tx^{(i)})+(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})}(-g'(\theta^Tx^{(i)}))] \\&=\sum_{i=1}^m[y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}g(\theta^Tx^{(i)})(1-g(\theta^Tx^{(i)}))\frac{\partial{(\theta^Tx^{(i)})}}{\theta_j}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})}g(\theta^Tx^{(i)})\\& (1-g(\theta^Tx^{(i)}))\frac{\partial{(\theta^Tx^{(i)})}}{\theta_j}] \\&=\sum_{i=1}^m[y^{(i)}(1-g(\theta^Tx^{(i)}))x^{(i)}_j-(1-y^{(i)})g(\theta^Tx^{(i)})x^{(i)}_j] \\&=\sum_{i=1}^m[y^{(i)}(1-g(\theta^Tx^{(i)}))-(1-y^{(i)})g(\theta^Tx^{(i)})]x^{(i)}_j \\&=\sum_{i=1}^m[y^{(i)}-g(\theta^Tx^{(i)})]x^{(i)}_j \\&=\sum_{i=1}^m[y^{(i)}-h_{\theta}(x^{(i)})]x^{(i)}_j\end{aligned}</script><p>最大化似然函数, 则批量梯度上升法可以采用:</p><script type="math/tex; mode=display">\theta_j=\theta_j+\alpha\sum_{i=1}^m[y^{(i)}-h_{\theta}(x^{(i)})]x^{(i)}_j</script><p>(对于目标函数 $J(\theta)$ 而言则是梯度下降法)</p><h3 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h3><h4 id="指数族"><a href="#指数族" class="headerlink" title="指数族"></a>指数族</h4><script type="math/tex; mode=display">p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))</script><p>$\eta$称为自然参数,$T(y)$ 称为充分统计量,线性回归和逻辑回归中, $T(y)=y$, $a(\eta)$ 称为对数分割函数.</p><h4 id="广义线性模型的三个假设"><a href="#广义线性模型的三个假设" class="headerlink" title="广义线性模型的三个假设"></a>广义线性模型的三个假设</h4><ul><li>$y|x;\theta \sim ExponentialFamily(\eta)$</li><li>给定$x$预测 $T(y)$ 的期望值, 也就是 $h(x)=E[T(y)|x]$, 对于线性回归和逻辑回归而言, $h(x)=E[y|x]$</li><li>$\eta=\theta^Tx$, 如果是 $\eta$ 为向量, 那么 $\eta_i=\theta_i^Tx$</li></ul><h4 id="再议线性回归"><a href="#再议线性回归" class="headerlink" title="再议线性回归"></a>再议线性回归</h4><p>在线性回归中, 我们知道$y$是连续值, 我们直接假设$y\sim\mathcal N(\mu,\sigma^2)$, 由于线性回归的模型参数与 $\sigma^2$ 无关, 我们可以直接假设 $y\sim\mathcal N(\mu, 1)$, 那么</p><script type="math/tex; mode=display">\begin{aligned}P(y;\mu)&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{(y-\mu)^2}{2}) \\&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{y^2+\mu^2-2\mu y}{2}) \\&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{y^2}{2}+\mu y-\frac{\mu^2}{2}) \\&=\frac{1}{\sqrt{2\pi}}\exp(-\frac{y^2}{2})\cdot\exp(\mu y-\frac{\mu^2}{2})\end{aligned}</script><p>对比指数族分布的公式: $p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))$</p><p>不难看出:</p><script type="math/tex; mode=display">b(y)=\frac{1}{\sqrt{2\pi}}\exp(-\frac{y^2}{2})</script><script type="math/tex; mode=display">\eta=\mu</script><script type="math/tex; mode=display">T(y)=y</script><script type="math/tex; mode=display">a(\eta)=-\frac{\mu^2}{2}</script><p>所以高斯分布实际上也属于指数族，根据广义线性模型的假设，第一个假设得到满足, 第二个假设为:</p><script type="math/tex; mode=display">h(x)=E[y|x]=\mu=\eta</script><p>第三个假设为：</p><script type="math/tex; mode=display">h(x)=\eta=\theta^Tx</script><p>据此推导出了该广义线性模型，也就是线性回归的模型.</p><h4 id="再议逻辑回归"><a href="#再议逻辑回归" class="headerlink" title="再议逻辑回归"></a>再议逻辑回归</h4><p>同理, 在二分类问题中常有</p><script type="math/tex; mode=display">y\in \{0, 1\}</script><p>如果用逻辑回归来处理该二分类问题，可以用一个伯努利分布来表示, 也就是$y\sim Bernoulli(\phi)$，可以写成:</p><script type="math/tex; mode=display">P(y=1;\phi)=\phi</script><script type="math/tex; mode=display">P(y=0;\phi)=1-\phi</script><p>或者</p><script type="math/tex; mode=display">P(y;\phi)=\phi^y(1-\phi)^{1-y}</script><p>因此</p><script type="math/tex; mode=display">\begin{aligned}P(y;\phi)&=\phi^y(1-\phi)^{1-y} \\&=\exp\log(\phi^y(1-\phi)^{1-y}) \\&=\exp(ylog\phi+(1-y)\log(1-\phi)) \\&=\exp(ylog\phi+\log(1-\phi)-y\log(1-\phi)) \\&=\exp(\log\frac{\phi}{1-\phi}y+\log(1-\phi))\end{aligned}</script><p>对比指数族的公式，可以得到:</p><script type="math/tex; mode=display">b(y)=1</script><script type="math/tex; mode=display">\eta=\log\frac{\phi}{1-\phi}</script><script type="math/tex; mode=display">T(y)=y</script><script type="math/tex; mode=display">a(\eta)=\log(1-\phi)</script><p>可以反解出:</p><script type="math/tex; mode=display">\phi=\frac{e^{\eta}}{1+e^{\eta}}=\frac{e^{\eta}\cdot e^{-\eta}}{(1+e^{\eta})\cdot e^{-\eta}}=\frac{1}{1+e^{-\eta}}</script><p>由此可知, 伯努利分布也是也属于指数族, 满足第一点假设, 根据第二点假设得:</p><script type="math/tex; mode=display">h(x)=E[y|x]=\phi=\frac{1}{1+e^{-\eta}}</script><p>并根据第三点假设得到:</p><script type="math/tex; mode=display">h(x)=\frac{1}{1+e^{-\eta}}=\frac{1}{1+e^{-\theta^Tx}}</script><p>据此推导出了该广义线性模型，也就是逻辑回归的模型.</p><h4 id="softmax回归"><a href="#softmax回归" class="headerlink" title="softmax回归"></a>softmax回归</h4><p>如果一个分类问题的类别在3个以上, 这个时候就要考虑多项式分布建模了, 可以用 $k - 1$ 个不同参数来描述:</p><script type="math/tex; mode=display">p(y=i;\phi)=\phi_i,i=1,2,...,k-1</script><script type="math/tex; mode=display">p(y=k;\phi)=1-\sum_{i=1}^{k-1}\phi_i</script><p>定义 $T(y) \in R^{k - 1}$:</p><script type="math/tex; mode=display">T(1)=\begin{bmatrix}1 \\0 \\0 \\\vdots \\0\end{bmatrix},T(2)=\begin{bmatrix}0 \\1 \\0 \\\vdots \\0\end{bmatrix},T(1)=\begin{bmatrix}0 \\0 \\1 \\\vdots \\0\end{bmatrix},T(k-1)=\begin{bmatrix}0 \\0 \\0 \\\vdots \\1\end{bmatrix},T(k)=\begin{bmatrix}0 \\0 \\0 \\\vdots \\0\end{bmatrix}</script><p>也就是</p><script type="math/tex; mode=display">{T(y)}_i=1\{y=i\}</script><p>或者</p><script type="math/tex; mode=display">T(y)=\begin{bmatrix}1\{y=1\} \\1\{y=2\} \\\vdots \\1\{y=k-1\} \\\end{bmatrix}</script><p>将该多项式分布写成指数分布的形式:</p>$$\begin{aligned}P(y;\phi)=&\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}...\phi_k^{1\{y=k\}} \\&=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}...\phi_k^{1-\sum_{i=1}^{k-1}1\{y=1\}} \\&=\phi_1^{{T(y)}_1}\phi_2^{{T(y)}_2}...\phi_k^{1-\sum_{i=1}^{k-1}{T(y)}_i} \\&=\exp\log\phi_1^{{T(y)}_1}\phi_2^{{T(y)}_2}...\phi_k^{1-\sum_{i=1}^{k-1}{T(y)}_i} \\&=\exp({{T(y)}_1}\log\phi_1+{{T(y)}_2}\log\phi_2+...+(1-\sum_{i=1}^{k-1}{T(y)}_i)\log(\phi_k)) \\&= \exp({{T(y)}_1}\log\frac{\phi_1}{\phi_k}+{{T(y)}_2}\log\frac{\phi_2}{\phi_k}+...+{T(y)}_{k-1}\log\frac{\phi_{k-1}}{\phi_k}+\log{\phi_k}) \\&= 1\cdot\exp(\begin{bmatrix}\log\frac{\phi_1}{\phi_k} \\\log\frac{\phi_2}{\phi_k} \\\vdots \\\log\frac{\phi_{k-1}}{\phi_k}\end{bmatrix}^T\begin{bmatrix}{T(y)}_1 \\{T(y)}_2 \\\vdots \\{T(y)}_{k-1} \\\end{bmatrix} + \log{\phi_k})\end{aligned}$$<p>由此得到:</p><script type="math/tex; mode=display">\eta=\begin{bmatrix}    \log\frac{\phi_1}{\phi_k} \\    \log\frac{\phi_2}{\phi_k} \\    \vdots \\    \log\frac{\phi_{k-1}}{\phi_k}\end{bmatrix}</script><script type="math/tex; mode=display">a(\eta)=-\log(\phi_k)</script><script type="math/tex; mode=display">b(y)=1</script><p>特别地，定义 $\eta_k=\log\frac{\phi_k}{\phi_k}=0$, 又由于链接函数:</p><script type="math/tex; mode=display">\eta_i=\log\frac{\phi_i}{\phi_k}</script><p>所以:</p><script type="math/tex; mode=display">e^{\eta_i}=\frac{\phi_i}{\phi_k}</script><script type="math/tex; mode=display">\phi_ke^{\eta_i}=\phi_i</script><script type="math/tex; mode=display">\phi_k\sum_{i=1}^ke^{\eta_i}=\sum_{i=1}^k\phi_i=1</script><p>也就是:</p><script type="math/tex; mode=display">\phi_k = \frac{1}{\sum_{i=1}^ke^{\eta_i}}</script><p>代回第二个式子, 最终将 $\eta$ 映射到 $\phi$ 得到softmax函数:</p><script type="math/tex; mode=display">\phi_i = \frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}</script><p>定义 $\theta_k=\vec{0}$, 这样就有 $\eta_k=\theta_kx=0$了, 所以:</p><script type="math/tex; mode=display">\begin{aligned}p(y=i|x;\theta)&=\phi_i \\&=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}} \\&=\frac{e^{\theta_i^Tx}}{\sum_{j=1}^ke^{\theta^T_jx}}\end{aligned}</script><p>这就解决了</p><script type="math/tex; mode=display">y \in \{1,2,...,k\}</script><p>的多分类问题.<br>根据最后一点假设，可以得到:</p><script type="math/tex; mode=display">\begin{aligned}h_{\theta}(x)&=E[T(y)|x;\theta] \\&=E\begin{bmatrix}1\{y=1\} & \\1\{y=2\} &\\\vdots & x;\theta\\1\{y=k-1\} &\\\end{bmatrix} \\&=\begin{bmatrix}\phi_1\\\phi_2 \\\vdots \\\phi_{k-1}\end{bmatrix} \\&=\begin{bmatrix}\frac{e^{\theta_1^Tx}}{\sum_{j=1}^ke^{\theta^T_jx}} \\\frac{e^{\theta_2^Tx}}{\sum_{j=1}^ke^{\theta^T_jx}} \\\vdots \\\frac{e^{\theta_{k-1}^Tx}}{\sum_{j=1}^ke^{\theta^T_jx}}\end{bmatrix}\end{aligned}</script>尽管 $h_{\theta}(x)$只有 $k-1$ 维, 但是 $p(y=k|x;\theta)$ 可以由 $1 - \sum_{i=1}^{k-1}\phi_i$得到, 另外, 似然函数为:<script type="math/tex; mode=display">\begin{aligned}l(\theta)&=\log\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\&=\sum_{i=1}^m\log p(y^{(i)}|x^{(i)};\theta) \\&=\sum_{i=1}^m\log(\prod_{l=1}^k\frac{e^{\theta_{l}^Tx}}{\sum_{j=1}^ke^{\theta^T_jx}})^{1(y^{(i)}=l)}\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode笔记(五)</title>
      <link href="/2019/09/27/leetcode-note-5/"/>
      <url>/2019/09/27/leetcode-note-5/</url>
      
        <content type="html"><![CDATA[<h3 id="存在重复元素III-220"><a href="#存在重复元素III-220" class="headerlink" title="存在重复元素III (220)"></a>存在重复元素III (220)</h3><p>给定一个整数数组，判断数组中是否有两个不同的索引 $i$ 和 $j$，使得 <code>nums [i]</code> 和 <code>nums [j]</code> 的差的绝对值最大为 $t$，并且 $i$ 和 $j$ 之间的差的绝对值最大为$k$.</p><pre><code>输入: nums = [1,2,3,1], k = 3, t = 0输出: true输入: nums = [1,0,1,1], k = 1, t = 2输出: true输入: nums = [1,5,9,1,5,9], k = 2, t = 3输出: false</code></pre><p>原题链接：<a href="https://leetcode-cn.com/problems/contains-duplicate-iii" target="_blank" rel="noopener">https://leetcode-cn.com/problems/contains-duplicate-iii</a></p><h4 id="滑动窗口-平衡二叉搜索树"><a href="#滑动窗口-平衡二叉搜索树" class="headerlink" title="滑动窗口+平衡二叉搜索树"></a>滑动窗口+平衡二叉搜索树</h4><p>我们首先来看看题目给出的两个主要约束:</p><script type="math/tex; mode=display">|i - j| \leq k \\|n_i-n_j| \leq t</script><p>由于数组的索引$i$,$j$是有序的(要么$i$比较大, 要么$j$比较大, 两种情况有一个成立, 并且也是对称的),不妨假设$j \geq i$, 那么第一个约束条件就转换为了:</p><script type="math/tex; mode=display">i \leq j \leq i + k</script><p>仔细一看, 由于$j \in [i, i + k]$, 所以题目可以转换为在固定长度为$l = i + k - i + 1 = k + 1$的滑动窗口内, 是否存在两个数$n_i$和$n_j$, 使得 $|n_i - n_j| \leq t$的存在性问题.$i$随着数组的遍历从$0$开始. 这个约束不等式可以进一步拆分成下面两个:</p><script type="math/tex; mode=display">n_i - t \leq n_j < n_i \\n_i \leq n_j \leq n_i + t</script><p>从这两个不等式不难看出, 这是一个在以$n_i$为中心, $t$为半径的线段上搜索$n_j$是否存在的问题, $n_j$可以落在中心$n_i$的左半边 $[n_i-t, n_i)$ 或者右半边 $[n_i, n_i + t]$ 上, 当然, 这还需要满足的一个前提是滑动窗口的大小为$k + 1$.</p><p>对于搜索问题, 我们可以联想到哪些? 线性查找, 二分查找, 二叉搜索树, 哈希表?</p><ol><li>线性表, 线性表存在的问题在于, 滑动窗口每移动一次(即$i$前进一次), 就得遍历长度为$k + 1$的窗口内的所有元素, 时间复杂度为$O(nk)$;</li><li>二分查找, 然而数组不一定有序所以不能采用此方法;</li><li>哈希表, 哈希表可以高效增删元素和定位一个具体元素, 但不适合这种范围查找(需要比较).</li></ol><p>剩下二叉搜索树, 它的适用场景就体现出来了:</p><ul><li>能够动态地存储和删除数据, 符合“滑动窗口”元素是动态变化, 虽然它没有哈希表那么高效;</li><li>其顺序性能够帮助我们搜索落在 $[n_i - t, n_i + t]$ 这个区间内的元素.</li></ul><p>在JAVA中, <code>Set</code>有一个具体的子类<code>TreeSet</code>, 它是用红黑树实现的平衡二叉搜索树. 平衡二叉搜索树的主要特点在于:</p><ul><li>每个结点的值都比它的左子树上的所有结点的值大, 但比它的右子树上的所有结点的值小;</li><li>每个结点的平衡因子不会超过2, 也就是它的左右子树的高度差不会超过2.</li></ul><p><code>TreeSet</code>有两个有用的成员方法:</p><pre><code class="lang-Java">K ceilingKey(K key); // 返回大于key的所有数中最小的一个K floorKey(K key); // 返回小于key的所有数种最大的一个</code></pre><p>我们可以采用<strong>边构造二叉搜索树边查找</strong>的方法来实现整个算法.</p><ol><li>对于中心$n_i$的左半边 $[n_i-t, n_i)$, 可以用<code>ceilingKey(nums[i] - t)</code>查找是否存在这样的$n_j$;  </li><li>对于$n_i$的右半边 $[n_i, n_i + t]$, 可以用<code>floorKey(nums[i] + t)</code>查找是否存在这样的$n_j$;</li><li>否则直接将当前的<code>nums[i]</code>加入<code>TreeSet</code>. 需要注意的一点是, 由于滑动窗口大小固定为$k + 1$, 随着遍历的进行, $i$从0开始增大, 也就是滑动窗口开始向右扩张, 当扩张到$i=k$时就需要删除掉最开始加入<code>TreeSet</code>的元素<code>nums[i - k]</code>.</li></ol><p>具体的实现代码如下:</p><pre><code class="lang-Java">public boolean containsNearbyAlmostDuplicate(int[] nums, int k, int t) {  TreeSet&lt;Long&gt; BST = new TreeSet&lt;Long&gt;();  for(int i = 0; i &lt; nums.length; i ++) {    Long ceiling = BST.ceiling((long) (nums[i] - t));    if(ceiling != null &amp;&amp; ceiling &lt;= nums[i])      return true;    Long floor = BST.floor((long) (nums[i] + t));    if(floor != null &amp;&amp; floor &gt;= nums[i])      return true;    BST.add((long) nums[i]);    if(i &gt;= k) {      BST.remove((long)(nums[i - k]));    }  }  return false;}</code></pre><ul><li><p>时间复杂度：$O(n\log(\min(n,k)))$,<br>我们需要遍历这个长度为n的数组。对于每次遍历，在 BST 中<code>搜索</code>，<code>插入</code> 或者 <code>删除</code>, 都需要花费$O(\log \min(k, n))$ 的时间.</p></li><li><p>空间复杂度：$O(\min(n,k))$,<br>空间复杂度由 BST 的大小决定，其大小的上限由 $k$ 和 $n$ 共同决定.</p></li></ul><h4 id="桶"><a href="#桶" class="headerlink" title="桶"></a>桶</h4><p>我们可以把数组种的数分配到一系列桶中, 设置每个桶的大小为$t$, 也就是得到了 $[0, t]$, $[t + 1, 2t + 1], …$ 这一组桶, 将元素 $n_i$放入编号为 $\frac{n_i}{t + 1}$ 的桶中, 可以注意到:</p><ul><li>同一个桶中的任意两个元素之差绝对不会超过$t$ (比如第一个桶, 最大值$t$和最小值$0$之差就为$t$);</li><li>相邻桶之间的两个元素之差可能不会超过$t$;<br>还有一件值得注意的事，这个问题和桶排序的不同之处在于每次我们的桶里只需要包含最多一个元素就可以了，因为如果任意一个桶中包含了两个元素，那么这也就是意味着这两个元素是 足够接近的 了，这时候我们就直接得到答案了。因此，我们只需使用一个标签为桶序号的散列表就可以了。</li></ul><p>另外, 同样, 随着遍历的进行, 当 $i \geq k$时, 需要删除最开始存放<code>nums[i - k]</code>的桶<code>nums[i - k] / (t + 1)</code>, 这样就能保证遍历到第<code>i+1</code>个元素时，全部桶中元素的索引最小值是<code>i-k+1</code>.</p><p>代码如下：</p><pre><code class="lang-Java">// Get the ID of the bucket from element value x and bucket width w// In Java, `-3 / 5 = 0` and but we need `-3 / 5 = -1`.public long getID(long x, long w) {    return x &lt; 0 ? (x + 1) / w - 1 : x / w;}public boolean containsNearbyAlmostDuplicate(int[] nums, int k, int t) {    if (t &lt; 0) return false;    Map&lt;Long, Long&gt; d = new HashMap&lt;&gt;();    long w = (long)t + 1;    for (int i = 0; i &lt; nums.length; ++i) {        long m = getID(nums[i], w);        // check if bucket m is empty, each bucket may contain at most one element        if (d.containsKey(m))            return true;        // check the neighbor buckets for almost duplicate        if (d.containsKey(m - 1) &amp;&amp; Math.abs(nums[i] - d.get(m - 1)) &lt; w)            return true;        if (d.containsKey(m + 1) &amp;&amp; Math.abs(nums[i] - d.get(m + 1)) &lt; w)            return true;        // now bucket m is empty and no almost duplicate in neighbor buckets        d.put(m, (long)nums[i]);        if (i &gt;= k) d.remove(getID(nums[i - k], w));    }    return false;}</code></pre><ul><li><p>时间复杂度：$O(n)$,<br>对于这 $n$ 个元素中的任意一个元素来说，我们最多只需要在散列表中做三次<code>搜索</code>，一次<code>插入</code>和一次<code>删除</code>。这些操作是常量时间复杂度的.因此，整个算法的时间复杂度为 $O(n)$。</p></li><li><p>空间复杂度：$O(\min(n, k))$,<br>需要开辟的额外空间取决了散列表的大小，其大小跟它所包含的元素数量成线性关系。散列表的大小的上限同时由$n$和$k$决定。因此，空间复杂度为 $O(\min(n, k))$.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode笔记(四)</title>
      <link href="/2019/09/23/leetcode-note-4/"/>
      <url>/2019/09/23/leetcode-note-4/</url>
      
        <content type="html"><![CDATA[<h3 id="H指数"><a href="#H指数" class="headerlink" title="H指数"></a>H指数</h3><p>给定一位研究者论文被引用次数的数组（被引用次数是非负整数）。编写一个方法，计算出研究者的 h 指数。</p><p>h 指数的定义: “h 代表“高引用次数”（high citations），一名科研人员的 h 指数是指他（她）的 （N 篇论文中）有（原文为最多有, 实际上没有英文原题没有体现出”最多”二字, 属于题目翻译错误） h 篇论文分别被引用了至少 h 次。（其余的 N - h 篇论文每篇被引用次数不多于 h 次。）”</p><p>原题链接：<a href="https://leetcode-cn.com/problems/h-index" target="_blank" rel="noopener">https://leetcode-cn.com/problems/h-index</a></p><pre><code>输入: citations = [3,0,6,1,5]输出: 3解释: 给定数组表示研究者总共有 5 篇论文，每篇论文相应的被引用了 3, 0, 6, 1, 5 次。     由于研究者有 3 篇论文每篇至少被引用了 3 次，其余两篇论文每篇被引用不多于 3 次，所以她的 h 指数是 3。</code></pre><h4 id="一般思路"><a href="#一般思路" class="headerlink" title="一般思路"></a>一般思路</h4><p>首先将论文引用次数的数组降序排列, 如果采用比较快速的排序算法(如快速排序, 堆排序, 归并排序), 其时间复杂度为$O(n\log{n})$.现在给出如下经过排序的数组:</p><div class="table-container"><table><thead><tr><th style="text-align:center">$i$</th><th style="text-align:center">0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">6</th></tr></thead><tbody><tr><td style="text-align:center">$article\;id$</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">5</td><td style="text-align:center">6</td><td style="text-align:center">7</td></tr><tr><td style="text-align:center">$citations$</td><td style="text-align:center">10</td><td style="text-align:center">9</td><td style="text-align:center">5</td><td style="text-align:center">3</td><td style="text-align:center">3</td><td style="text-align:center">2</td><td style="text-align:center">1</td></tr></tbody></table></div><p>从h指数的定义不难看出, 它是一个简单的一次函数, 其横轴的意义为论文的编号/数目, 纵轴表示引用次数, 如下图中的蓝色直线所示, 例如点 $(5, 5)$ 就表示有$5$篇文章至少被引用了$5$次. 由于$ID = i + 1$, 所以点 $(i + 1, i + 1)$ 表示有 $i + 1\;(1\sim{i + 1})$ 篇文章至少被引用了 $i + 1$ 次.</p><p><img src="/img/hindex.png"></p><p>我们把数组中每篇文章的编号极其对应的引用次数也绘制到上图中(红色圆点), 由于是降序排列, 我们只要找到最大的$i$使得$citations_{i} \geq i + 1$(或者$citations_i &gt; i$)就可以了, 此时H指数的值为: $H = i + 1$.</p><pre><code class="lang-Java">public int hIndex(int[] citations) {    // 此处为升序排列    Arrays.sort(citations);    int i = 0;    while(i &lt; citations.length &amp;&amp; citations[citations.length - 1 - i] &gt; i) {      i ++;    }    return i;}</code></pre><h4 id="计数排序法"><a href="#计数排序法" class="headerlink" title="计数排序法"></a>计数排序法</h4><p>H指数是存在最大值的, 如果给出$n$篇文章, 那么H指数的极限就是$n$篇文章至少被引用了$n$次, 这样一来可以得出的结论就是:</p><blockquote><p>H指数不会超过论文的总数.</p></blockquote><p>那么实际上如果给出一组文章引用次数的数组, 我们就可以将其中引用次数超过文章总数的值直接下调为文章总数，这样修改并不会影响最后的结果.比如给出下面的数组</p><script type="math/tex; mode=display">citations=[1,3,2,3,100]</script><p>文章总数$n=5$, 因此可以修改为:</p><script type="math/tex; mode=display">citations=[1,3,2,3,5]</script><p>先来看看计数排序的结果. 如果不了解计数排序, 可以先查看<a href="/2019/09/22/counting-sort/">此处</a>.</p><div class="table-container"><table><thead><tr><th style="text-align:center">引用次数$k$</th><th style="text-align:center">0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th></tr></thead><tbody><tr><td style="text-align:center">仅引用$k$次的文章计数$count$</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">至少引用$k$次的文章计数$s_k$</td><td style="text-align:center">5</td><td style="text-align:center">5</td><td style="text-align:center">4</td><td style="text-align:center">3</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr></tbody></table></div><p>根据H指数的定义: $h$篇论文分别被引用了至少$h$次. 如果修改为有$s_k$篇论文分别被至少引用了$k$次, 那么必须满足$k \leq s_k$才有<strong>可能</strong>使得H指数为$k$. 这是因为$s_k$表示累计文章总数, 又因为H指数不会超过文章总数, 那么当前的引用次数$k$也不会超过当前的累计文章总数, 所以$k \leq s_k$, 观察表格也能得出这样的结论. 而现在我们要找到这样一个最大的$k$才满足要求.在这个表格中, 显然:</p><script type="math/tex; mode=display">h = arg\max_{k \leq s_k}{(k)}=3</script><p>可以采用<strong>从后往前扫描</strong>的方法快速得到结果, 代码如下:</p><pre><code class="lang-Java">public int hIndex(int[] citations) {    int n = citations.length;    int[] papers = new int[n + 1];    // 计数    for (int c: citations)        papers[Math.min(n, c)]++;    // 找出最大的 k    int k = n;    for (int s = papers[n]; k &gt; s; s += papers[k])        k--;    return k;}</code></pre><p>经过优化, 算法的时间复杂度和空间复杂度都被降低到了$O(n)$.</p>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计数排序</title>
      <link href="/2019/09/22/counting-sort/"/>
      <url>/2019/09/22/counting-sort/</url>
      
        <content type="html"><![CDATA[<h3 id="与基于比较的排序算法的区别"><a href="#与基于比较的排序算法的区别" class="headerlink" title="与基于比较的排序算法的区别"></a>与基于比较的排序算法的区别</h3><p>一般的基于比较的排序算法的最好平均时间复杂度可以达到 $O(n\log{n})$ 的复杂度, 但是计数排序的时间复杂度可以达到$O(n)$. 但是计数排序在使用方面有个限制，即排序的数组中的元素必须在一定的范围内.</p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><p>给定数组$a$</p><div class="table-container"><table><thead><tr><th>$index$</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>$value$</td><td>3</td><td>0</td><td>2</td><td>3</td><td>1</td></tr></tbody></table></div><h4 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a><strong>第一步</strong></h4><p>假设代排序数组为$a$, 获取数组中最大和最小的元素, 分别记为 $\max{(a)}$ 和 $\min{(a)}$, 开辟数组$c$, 空间大小为: $\max{(a)}-\min{(a)} + 1$, 从给出的例子来看, 最大值和最小值分别为3和0, 所以开辟的数组大小为4;</p><h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a><strong>第二步</strong></h4><p>$c$ 中记录$a$中每个元素出现的次数:</p><div class="table-container"><table><thead><tr><th>$index$</th><th>0</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>$value$</td><td>1</td><td>1</td><td>1</td><td>2</td></tr></tbody></table></div><h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a><strong>第三步</strong></h4><p>修改$c$, 让$c$中的每个元素$c_i$记录不超过$i$的元素的个数, 也就是当前元素累加前一个元素:</p><script type="math/tex; mode=display">c_{i}=c_{i-1}+1</script><p>得到的结果为:</p><div class="table-container"><table><thead><tr><th>$index$</th><th>0</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>$value$</td><td>1</td><td>2</td><td>3</td><td>5</td></tr></tbody></table></div><h4 id="第四步"><a href="#第四步" class="headerlink" title="第四步"></a><strong>第四步</strong></h4><p>根据数组$c$得到排序后的数组$b$.</p><div class="table-container"><table><thead><tr><th style="text-align:center">$a_{index}$</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td style="text-align:center">$a$</td><td><strong>3</strong></td><td>0</td><td>2</td><td>3</td><td>1</td></tr><tr><td style="text-align:center">$c_{index}:=a_i-\min{(a)}$</td><td>0</td><td>1</td><td>2</td><td><strong>3</strong></td><td>-</td></tr><tr><td style="text-align:center">$c:=count(a_i)$</td><td>1</td><td>2</td><td>3</td><td><strong>5</strong></td><td>-</td></tr><tr><td style="text-align:center">$b_{index}$</td><td>1</td><td>2</td><td>3</td><td>4</td><td><strong>5</strong></td></tr><tr><td style="text-align:center">$b$</td><td></td><td></td><td></td><td></td><td><strong>3</strong></td></tr></tbody></table></div><p>这是怎么确定$b_5$是$3$的呢?注意加粗字体部分，它们实际上是串联在一起的, 前5行自上往下串成一条线:</p><script type="math/tex; mode=display">0 \rightarrow \mathbf{3} \rightarrow 3 \rightarrow 5 \rightarrow \mathbf{5}</script><p>注意数组$c$是累加的结果, 所以$b_5=3$ (注意上面串线加粗的$\mathbf{3}$和$\mathbf{5}$), 经过这一步计算后, 数组$c$会发生一些变化, 也就是不超过3的累加计数5需要减去1, 更新为4, 因为有一个3已经找到了它的位置.</p><div class="table-container"><table><thead><tr><th style="text-align:center">$a_{index}$</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td style="text-align:center">$a$</td><td>3</td><td>0</td><td>2</td><td>3</td><td>1</td></tr><tr><td style="text-align:center">$c_{index}:=a_i-\min{(a)}$</td><td>0</td><td>1</td><td>2</td><td>3</td><td>-</td></tr><tr><td style="text-align:center">$c:=count(a_i)$</td><td>1</td><td>2</td><td>3</td><td><strong>4</strong></td><td>-</td></tr><tr><td style="text-align:center">$b_{index}$</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td style="text-align:center">$b$</td><td></td><td></td><td></td><td></td><td>3</td></tr></tbody></table></div><p>接下来采用同样的方法进行”串线”:</p><script type="math/tex; mode=display">1 \rightarrow \mathbf{0} \rightarrow 0 \rightarrow 1 \rightarrow \mathbf{1}</script><div class="table-container"><table><thead><tr><th style="text-align:center">$a_{index}$</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td style="text-align:center">$a$</td><td>3</td><td><strong>0</strong></td><td>2</td><td>3</td><td>1</td></tr><tr><td style="text-align:center">$c_{index}:=a_i-\min{(a)}$</td><td><strong>0</strong></td><td>1</td><td>2</td><td>3</td><td>-</td></tr><tr><td style="text-align:center">$c:=count(a_i)$</td><td><strong>1</strong></td><td>2</td><td>3</td><td>4</td><td>-</td></tr><tr><td style="text-align:center">$b_{index}$</td><td><strong>1</strong></td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td style="text-align:center">$b$</td><td><strong>0</strong></td><td></td><td></td><td></td><td>3</td></tr></tbody></table></div><p>计算得到$b_1=1$, 同时不超过1的累加计数1需要减去1, 更新为0, 因为数字0找到了它的位置.</p><div class="table-container"><table><thead><tr><th style="text-align:center">$a_{index}$</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td style="text-align:center">$a$</td><td>3</td><td>0</td><td>2</td><td>3</td><td>1</td></tr><tr><td style="text-align:center">$c_{index}:=a_i-\min{(a)}$</td><td>0</td><td>1</td><td>2</td><td>3</td><td>-</td></tr><tr><td style="text-align:center">$c:=count(a_i)$</td><td><strong>0</strong></td><td>2</td><td>3</td><td>4</td><td>-</td></tr><tr><td style="text-align:center">$b_{index}$</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td style="text-align:center">$b$</td><td>0</td><td></td><td></td><td></td><td>3</td></tr></tbody></table></div><p>反复这样计算后得到的排序结果为:</p><div class="table-container"><table><thead><tr><th style="text-align:center">$a_{index}$</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td style="text-align:center">$a$</td><td>3</td><td>0</td><td>2</td><td>3</td><td>1</td></tr><tr><td style="text-align:center">$c_{index}:=a_i-\min{(a)}$</td><td>0</td><td>1</td><td>2</td><td>3</td><td>-</td></tr><tr><td style="text-align:center">$c:=count(a_i)$</td><td>1</td><td>2</td><td>3</td><td>4</td><td>-</td></tr><tr><td style="text-align:center">$b_{index}$</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td style="text-align:center">$b$</td><td>0</td><td>1</td><td>2</td><td>3</td><td>3</td></tr></tbody></table></div><p>其实上面的操作中, “串线”和累加计数减去1是重复进行的, 可以用下面的代码来实现:</p><pre><code class="lang-Java">// b[c[a[i] - min]]表示b从1开始计数// b[c[a[i] - min] - 1]表示b从0开始计数for(int i = 0; i &lt; a.length; i ++){    b[c[a[i] - min] - 1] = a[i];    c[a[i] - min]--;}</code></pre><p>完整的实现代码如下所示:</p><pre><code class="lang-Java">public static int[] solution(int[] a) {    int max = getMax(a), min = getMin(a);    int s = max - min + 1;    int[] c = new int[s];    int[] b = new int[a.length];    for(int i = 0; i &lt; a.length; i ++) {        c[a[i] - min] ++;    }    for(int i = 1; i &lt; c.length; i ++) {        c[i] = c[i] + c[i - 1];    }    for(int i = 0; i &lt; a.length; i ++) {        b[c[a[i] - min] - 1] = a[i];        c[a[i] - min] --;    }    return b;}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode笔记(三)</title>
      <link href="/2019/09/11/leetcode-note-3/"/>
      <url>/2019/09/11/leetcode-note-3/</url>
      
        <content type="html"><![CDATA[<h3 id="缺失的第一个正数-41"><a href="#缺失的第一个正数-41" class="headerlink" title="缺失的第一个正数(41)"></a>缺失的第一个正数(41)</h3><p>给定一个未排序的整数数组，找出其中没有出现的最小的正整数。<br>原题链接: <a href="https://leetcode-cn.com/problems/first-missing-positive/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/first-missing-positive/</a></p><pre><code>输入: [1,2,0]输出: 3输入: [3,4,-1,1]输出: 2输入: [7,8,9,11,12]输出: 1</code></pre><h4 id="空间复杂度为-O-n"><a href="#空间复杂度为-O-n" class="headerlink" title="空间复杂度为$O(n)$"></a>空间复杂度为$O(n)$</h4><ol><li>$nums \rightarrow set(nums)$;</li><li>$i$从$1$开始自增, 直到有出现在$set$中的元素为止, 这个元素就是第一个未出现的最小正整数;</li></ol><h4 id="原地算法-空间复杂度为-O-1"><a href="#原地算法-空间复杂度为-O-1" class="headerlink" title="原地算法, 空间复杂度为$O(1)$"></a>原地算法, 空间复杂度为$O(1)$</h4><p>假设数组$nums$的长度为$n$, 那么应当考虑:</p><ol><li>为了让数组中的正整数尽可能小，$nums$中本来的正整数应当是$1\sim n$(但实际上不是), 基于这一点，我们应该关注数组中那些位于$1\sim n$的数字;</li><li>假设数组$nums = [1, 2, 3, 4]$, 每个元素对应的索引依次为$0, 1, 2, 3$, 也就是$nums_i = i + 1$, 也就是说:<pre><code>由于nums[i] = i + 1;所以i = nums[i] - 1;所以nums[i] = nums[nums[i] - 1];</code></pre>那么如果$nums = [3, 4, -1, 1]$, 关注该数组中位于$1 \sim 4$的数, 分别为$1, 3, 4$，这些数应该放到索引为$0, 2, 3$的位置上，下面我们会介绍怎么通过交换去放这些数.</li></ol><p>第一次遍历数组$nums$</p><pre><code>i = 0, nums[0] != nums[nums[0] - 1] = nums[3 - 1] = nums[2], 因此交换索引0和索引2的数, 得到[-1, 4, 3, 1];i = 0, nums[0] = -1不在集合{1, 2, 3, 4}中, i ++;i = 1, nums[1] != nums[nums[1] - 1] = nums[4 - 1] = nums[3], 因此交换索引1和索引4的数, 得到[-1, 1, 3, 4];i = 1, nums[1] != nums[nums[1] - 1] = nums[1 - 1] = nums[0], 因此交换索引0和索引1的数, 得到[1, -1, 3, 4];i = 1, nums[1] = -1不在集合{1, 2, 3, 4}中, i ++;i = 2, nums[2] == nums[nums[2] - 1] = nums[3 - 1] = nums[2], i ++;i = 3, nums[3] == nums[nums[3] - 1] = nums[4 - 1] = nums[3], i ++;i = 4 &gt; n - 1 = 3，循环结束;</code></pre><p>第二次遍历数组$nums$</p><pre><code>一旦出现nums[i] != i + 1就返回i + 1, 明显元素-1就不符合，返回i + 1 = 2;</code></pre><p>再举个简单的例子，$nums = [5, 6, 7, 8]$</p><pre><code>由于数组长度为4, 每个元素都不在集合{1, 2, 3, 4}中, 第一遍循环不执行, 第二遍循环nums[0] != 0 + 1, 直接返回1;</code></pre><p>最后一个例子, $nums = [1, 2, 3, 4]$</p><pre><code>数组中的每个元素都在集合{1, 2, 3, 4}中且nums[i] = nums[nums[i] - 1], 所以第一遍循环也不执行;第二遍循环nums[i] == i + 1也全部成立, 这时候只要直接返回数组长度加1即可, 也就是4 + 1 = 5;</code></pre><p>代码如下:</p><pre><code>时间复杂度O(n)尽管循环有两次, 但是每交换一次，都有一个数回归到正确位置，所以两层循环最多交换n次，所以是O(n)不是O(n^2)额外空间为O(1)public static int firstMissingPositive(int[] nums) {    int n = nums.length;    for(int i = 0; i &lt; n; i ++) {        while(nums[i] &gt; 0 &amp;&amp; nums[i] &lt;= n &amp;&amp; nums[i] != nums[nums[i] - 1]) {            swap(nums, i, nums[i] - 1);        }    }    for(int i = 0; i &lt; n; i ++) {        if(nums[i] != i + 1)            return i + 1;    }    return n + 1;}public static void swap(int[] nums, int idx1, int idx2) {      if(idx1 == idx2)          return;      nums[idx1] = nums[idx1] + nums[idx2];      nums[idx2] = nums[idx1] - nums[idx2];      nums[idx1] = nums[idx1] - nums[idx2];}</code></pre><p>小注:</p><pre><code>不引入额外变量交换a和ba = a + b;b = a - b;a = a - b;</code></pre><h3 id="猜数字游戏-299"><a href="#猜数字游戏-299" class="headerlink" title="猜数字游戏(299)"></a>猜数字游戏(299)</h3><p>你正在和你的朋友玩 猜数字（Bulls and Cows）游戏：你写下一个数字让你的朋友猜。每次他猜测后，你给他一个提示，告诉他有多少位数字和确切位置都猜对了（称为“Bulls”, 公牛），有多少位数字猜对了但是位置不对（称为“Cows”, 奶牛）。你的朋友将会根据提示继续猜，直到猜出秘密数字。</p><p>请写出一个根据秘密数字和朋友的猜测数返回提示的函数，用 A 表示公牛，用 B 表示奶牛。</p><p>请注意秘密数字和朋友的猜测数都可能含有重复数字。</p><p>原题链接: <a href="https://leetcode-cn.com/problems/bulls-and-cows" target="_blank" rel="noopener">https://leetcode-cn.com/problems/bulls-and-cows</a></p><pre><code>输入: secret = &quot;1807&quot;, guess = &quot;7810&quot;输出: &quot;1A3B&quot;解释: 1 公牛和 3 奶牛。公牛是 8，奶牛是 0, 1 和 7。</code></pre><pre><code>输入: secret = &quot;1123&quot;, guess = &quot;0111&quot;输出: &quot;1A1B&quot;解释: 朋友猜测数中的第一个 1 是公牛，第二个或第三个 1 可被视为奶牛。</code></pre><pre><code>输入: secret = &quot;1122&quot;, guess = &quot;2211&quot;输出: &quot;0A4B&quot;</code></pre><h3 id="两个字典数组"><a href="#两个字典数组" class="headerlink" title="两个字典数组"></a>两个字典数组</h3><ol><li>用两个简单变量$a$和$b$来对公牛和母牛计数, 建立两个字典数组$d_1$和$d_2$, 字典数组的索引范围只能是$0\sim9$(整数位数), 因此数组长度可以固定为$10$;</li><li>遍历$s$, 如果$s_i = g_i$, $a\leftarrow a + 1$; 否则$d_1[s_i]\leftarrow d_1[s_i]+1$, $d_2[g_i]\leftarrow d_2[g_i]+1$;</li></ol><pre><code>以第二组样例s = 1123和g = 0111说明:s[0] != g[0], 所以d1[1] = 0 + 1 = 1, d2[0] = 0 + 1 = 1;s[1] == g[1], a = a + 1 = 1;s[2] != g[2], 所以d1[2] = 0 + 1 = 1, d2[1] = 0 + 1 = 1;s[3] != g[3], 所以d1[3] = 0 + 1 = 1, d2[1] = 1 + 1 = 2;所以d1 = {0, 1, 1, 1}, d2 = {1, 2, 0, 0};</code></pre><ol><li>遍历$d_1$, 如果$d_1[i] = d_2[i]$表示它们共同包含了整数$i$(但是位置不同), 取它们的最小值并加到$b$上即可.<script type="math/tex; mode=display">b \leftarrow b + min(d_1[i],  d_2[i])</script><pre><code>还是以第二组样例说明:d1[1] = 1, d2[1] = 2, 说明对于位置不同的1，s中出现了1次, g中出现了2次, 那么b = 0 + min(1, 2) = 1;其它数字没有重叠，结束执行。</code></pre>代码如下:<pre><code>public static String twoDictSol(String secret, String guess) {   int n = secret.length(), a = 0, b = 0;   int[] d1 = new int[10];   int[] d2 = new int[10];   for(int i = 0; i &lt; n; i ++) {     if(secret.charAt(i) == guess.charAt(i)) {       a ++;     }else {       d1[secret.charAt(i) - &#39;0&#39;] ++;       d2[guess.charAt(i) - &#39;0&#39;] ++;     }   }   for(int i = 0; i &lt; d1.length; i ++) {     if(d1[i] != 0 &amp;&amp; d2[i] != 0) {       b += Math.min(d1[i], d2[i]);     }   }   return a + &quot;A&quot; + b + &quot;B&quot;;}</code></pre></li></ol><h3 id="单字典数组或者哈希表"><a href="#单字典数组或者哈希表" class="headerlink" title="单字典数组或者哈希表"></a>单字典数组或者哈希表</h3><p>上面的方法中用到了两个字典数组, 现在可以缩减到一个, 可以把$d_2$去掉，直接用一个字典数组$d$来记录$s$中那些位置不同的数</p><script type="math/tex; mode=display">d[s_i] \leftarrow d[s_i] + 1\;\;\;if(s_i \neq g_i)</script><p>重新遍历一遍$s$, 如果$s_i \neq g_i \wedge d[g_i] \neq 0$, 则$d[g_i] \leftarrow d[g_i] - 1$, $b \leftarrow b + 1$;<br>这里$s_i \neq g_i$主要是去除掉符合位置相同，数值也相同的$g_i$, 这个是不能拿来检查的.</p><pre><code>以第二组样例s = 1123和g = 0111说明, d就是上个方法中的d1, d = {0, 1, 1, 1};s[0] != g[0]但是d[g[0]] = d[0] = 0, s中没有数字0;s[1] == g[1], 跳过;s[2] != g[2]但是d[g[2]] = d[1] = 1, d[1] = d[1] - 1 = 0, b = b + 1 = 1, s中有1个1，用完后s中就没有1了;s[3] != g[3]但是d[g[3]] = d[1] = 0, s中没有数字1了;</code></pre><p>上述方法可以将字典数组更换为哈希表, 其余的做法都是一样的.<br>代码如下:</p><pre><code>public static String singleDictSol(String secret, String guess) {      int n = secret.length(), a = 0, b = 0;      int[] d = new int[10];      for(int i = 0; i &lt; n; i ++) {        if(secret.charAt(i) == guess.charAt(i)) {          a ++;        }else {          d[secret.charAt(i) - &#39;0&#39;] ++;        }      }      for(int i = 0; i &lt; n; i ++) {        if(secret.charAt(i) != guess.charAt(i) &amp;&amp; d[guess.charAt(i) - &#39;0&#39;] != 0) {          d[guess.charAt(i) - &#39;0&#39;] --;          b ++;        }      }      return a + &quot;A&quot; + b + &quot;B&quot;;}</code></pre><h3 id="加油站-134"><a href="#加油站-134" class="headerlink" title="加油站(134)"></a>加油站(134)</h3><p>在一条环路上有 N 个加油站，其中第 i 个加油站有汽油 gas[i] 升。</p><p>你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。</p><p>如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1。<br>原始链接: <a href="https://leetcode-cn.com/problems/gas-station" target="_blank" rel="noopener">https://leetcode-cn.com/problems/gas-station</a></p><pre><code>输入:gas  = [1,2,3,4,5]cost = [3,4,5,1,2]输出: 3从 3 号加油站(索引为 3 处)出发，可获得 4 升汽油。此时油箱有 = 0 + 4 = 4 升汽油开往 4 号加油站，此时油箱有 4 - 1 + 5 = 8 升汽油开往 0 号加油站，此时油箱有 8 - 2 + 1 = 7 升汽油开往 1 号加油站，此时油箱有 7 - 3 + 2 = 6 升汽油开往 2 号加油站，此时油箱有 6 - 4 + 3 = 5 升汽油开往 3 号加油站，你需要消耗 5 升汽油，正好足够你返回到 3 号加油站。因此，3 可为起始索引。</code></pre><p>这个题目的关键点只有两个:</p><ol><li><p>如果$\sum{gas} &lt; \sum{cost}$, 环绕一圈是不可能的，此时将返回结果-1, 环行过程中油箱剩余总的油量为:</p><script type="math/tex; mode=display">totalTank = \sum{gas} - \sum{cost}</script><p>如果$totalTank &lt; 0$则返回-1.</p></li><li><p>对于第$i$个加油站, 如果$gas_i - cost_i &lt; 0$, 则不能从加油站$i$出发到达加油站$i+1$, 如果引入变量$curTank$来记录当前油箱的油量, 前面的描述可以一般化为如果<strong>某一个加油站$curTank &lt; 0$, 则这个加油站是不可达的</strong>, 此时可以把发生$curTank &lt; 0$的这个加油站作为下一次出发的起点, 并将$curTank$重置为0. </p><pre><code>比如路线A-C-B-D-..., 如果将A作为起点，而B处发生$curTank &lt; 0$, 那么此时就将B作为起点，重新环行.也就是, 如果不能把A作为起点到达B, 那么也不可能把A和B中间的某一点作为起点(比如C)并通过该点到达C.这不仅排除了A作为起点，同时把C作为起点也排除了, 所以可以跳过C选择B作为下一个检查的起点.</code></pre><p>代码如下:</p><pre><code>public static int canCompleteCircuit(int[] gas, int[] cost) {     int totalTank = 0, curTank = 0, startStation = 0;     for(int i = 0; i &lt; gas.length; i ++) {         totalTank += gas[i] - cost[i];         curTank += gas[i] - cost[i]; //执行完这一句就到了站点i + 1         if(curTank &lt; 0) {             curTank = 0;             startStation = i + 1;         }     }     return totalTank &gt;= 0 ? startStation: -1;}</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode笔记(二)</title>
      <link href="/2019/09/10/leetcode-note-2/"/>
      <url>/2019/09/10/leetcode-note-2/</url>
      
        <content type="html"><![CDATA[<h3 id="移除元素-27"><a href="#移除元素-27" class="headerlink" title="移除元素(27)"></a>移除元素(27)</h3><p>给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。<br>原题链接：<a href="https://leetcode-cn.com/problems/remove-element" target="_blank" rel="noopener">https://leetcode-cn.com/problems/remove-element</a></p><pre><code>给定 nums = [0,1,2,2,3,0,4,2], val = 2,函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。注意这五个元素可为任意顺序。你不需要考虑数组中超出新长度后面的元素。</code></pre><p>原地删除问题，也就是必须在O(1)额外空间的条件下删除数组中的元素，解决这类问题的固定套路是双指针法。</p><ol><li>设置快指针$j$和慢指针$i$;</li><li><strong>如果</strong>$nums_j = val$, 就递增$j$;</li><li><strong>只要</strong>$nums_j \neq val$, 则$nums_i \leftarrow nums_j$, 同时递增$i$和$j$;<pre><code class="lang-Java">for(int j = 0; j &lt; n; j ++){   if(nums[j] != val){     nums[i] = nums[j];     i ++;   }}return i;</code></pre></li></ol><h3 id="删除排序数组中的重复项-26"><a href="#删除排序数组中的重复项-26" class="headerlink" title="删除排序数组中的重复项(26)"></a>删除排序数组中的重复项(26)</h3><p>给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。<br>原题链接：<a href="https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array" target="_blank" rel="noopener">https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array</a></p><pre><code>给定 nums = [0,0,1,1,1,2,2,3,3,4],函数应该返回新的长度 5, 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4。你不需要考虑数组中超出新长度后面的元素。</code></pre><p>依旧是原地删除问题，同样可以采用双指针法解决。</p><ol><li>设置快指针$j$和慢指针$i$;</li><li><strong>如果</strong>$nums_j = nums_i$, 就递增$j$;</li><li><strong>只要</strong>$nums_j \neq nums_i$, 同时递增$i$和$j$, $nums_i \leftarrow nums_j$;<pre><code class="lang-Java">int i = 0;for(int j = 1; j &lt; n; j ++){   if(nums[j] != nums[i]){     i ++;     nums[i] = nums[j];   }}</code></pre></li></ol><h3 id="删除排序数组中的重复项-80"><a href="#删除排序数组中的重复项-80" class="headerlink" title="删除排序数组中的重复项(80)"></a>删除排序数组中的重复项(80)</h3><p>给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素最多出现两次，返回移除后数组的新长度。<br>原题链接：<a href="https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array-ii" target="_blank" rel="noopener">https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array-ii</a></p><pre><code>给定 nums = [0,0,1,1,1,1,2,3,3],函数应返回新长度 length = 7, 并且原数组的前五个元素被修改为 0, 0, 1, 1, 2, 3, 3 。你不需要考虑数组中超出新长度后面的元素。</code></pre><p>还是原地删除问题，继续采用双指针法解决。不同的是这次元素最多可以重复两次，可以设置一个标志变量$k$，并初始化$k = 0$，表示数组当前的元素只有一个，一旦出现了第二个，则$k = 1$，之后若再出现第三，第四个跟它相同的元素，$k$也不会再发生变化，直到遇到不同的元素为止，$k$重新清为0.</p><ol><li>设置快指针$j$, 慢指针$i$和标志变量$k$;</li><li><strong>如果</strong>$nums_j = nums_i$, 1) 如果$k = 0$, 递增$i$和$j$, $k \leftarrow 1$, $nums_i \leftarrow nums_j$, 2) 如果$k \neq 0$, 递增$j$;</li><li><strong>只要</strong>$nums_j \neq nums_i$, 递增$i$和$j$, $k \leftarrow 0$, $nums_i \leftarrow nums_j$;</li></ol><p>所以看伪代码会发现，$nums_j = nums_i \wedge k = 0$和$nums_j \neq nums_i$这两种情况的执行部分是基本一致的，可看成同一种情况。</p><pre><code class="lang-Java">int i = 0, k = 0;for(int j = 1; j &lt; nums.length; ++ j) {    if(nums[i] == nums[j]) {        if(k == 0) {            i ++;            nums[i] = nums[j];            k ++;        }    }else {        i ++;        nums[i] = nums[j];        k = 0;    }}return i + 1;</code></pre><h3 id="旋转数组"><a href="#旋转数组" class="headerlink" title="旋转数组"></a>旋转数组</h3><p>给定一个数组，将数组中的元素向右移动 k 个位置，其中 k 是非负数。<br>原题链接：<a href="https://leetcode-cn.com/problems/rotate-array" target="_blank" rel="noopener">https://leetcode-cn.com/problems/rotate-array</a></p><pre><code>输入: [1,2,3,4,5,6,7] 和 k = 3输出: [5,6,7,1,2,3,4]解释:向右旋转 1 步: [7,1,2,3,4,5,6]向右旋转 2 步: [6,7,1,2,3,4,5]向右旋转 3 步: [5,6,7,1,2,3,4]</code></pre><p>如果不采用原地算法(O(1))的额外空间，只需要用另外一个额外数组<code>a[]</code>来保存<code>nums[]</code>旋转后的结果即可，原来在位置<code>i</code>上的数，旋转<code>k</code>次后会来到<code>(i + k) % n</code>的位置，其中<code>n</code>是数组的长度，这有点像是循环队列.</p><pre><code>a[i] = nums[(i + k) % n]</code></pre><p>但是原题要求用原地算法，那么可以采用的一种简洁算法是：反转算法<br>当我们旋转数组<code>k</code>次时，那么就有<code>n % k</code>个尾部元素移动到了数组的首部，剩下的按顺序往后移动.<br>反转算法的做法是：</p><ul><li>先将整个数组反转</li><li>再反转前<code>k</code>个元素</li><li>最后反转后<code>n - k</code>个元素</li></ul><pre><code>原始数组                  : 1 2 3 4 5 6 7反转所有数字后             : 7 6 5 4 3 2 1反转前 k 个数字后          : 5 6 7 4 3 2 1反转后 n-k 个数字后        : 5 6 7 1 2 3 4</code></pre><p>代码如下：</p><pre><code>public static void rotate(int[] nums, int k) {    k %= nums.length;    reverse(nums, 0, nums.length - 1);    reverse(nums, 0, k - 1);    reverse(nums, k, nums.length - 1);}public static void reverse(int[] nums, int start, int end) {    for(int i =  start, j = end; i &lt; j; i ++, j --) {      int temp = nums[i];      nums[i] = nums[j];      nums[j] = temp;    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode笔记(一)</title>
      <link href="/2019/09/09/leetcode-note-1/"/>
      <url>/2019/09/09/leetcode-note-1/</url>
      
        <content type="html"><![CDATA[<h3 id="两数之和-1"><a href="#两数之和-1" class="headerlink" title="两数之和(1)"></a>两数之和(1)</h3><pre><code>1. 建立map: nums[i] -&gt; i;2. 判断target - nums[i]是否也在map中3. 如果是就返回(i, j)</code></pre><h3 id="两数相加-2"><a href="#两数相加-2" class="headerlink" title="两数相加(2)"></a>两数相加(2)</h3><pre><code>  5 -&gt; 2 -&gt; 3+ 6 -&gt; 1(-&gt; 0)= 1 -&gt; 4 -&gt; 3Node dh = new Node(), cur = dh;carry = 0;while(p != null || q != null){  x = p != null ? p.val: 0;  y = q != null ? q.val: 0;  sum = x + y + carry;  carry = sum / 10;  cur.next = new Node(sum % 10);  cur = cur.next;  if(p != null) p = p.next;  if(q != null) q = q.next;}if(carry &gt; 0)  cur.next = new Node(carry);return dh.next</code></pre><h3 id="无重复字符的最长子串-3"><a href="#无重复字符的最长子串-3" class="headerlink" title="无重复字符的最长子串(3)"></a>无重复字符的最长子串(3)</h3><pre><code>1. 建立map: s[j] -&gt; j + 1, ans = 0;2. 如果s[j]已经存在于map中，起点i可以直接跳跃到map[s[j]]也就是j + 1处，但要注意map中的旧状态还没有移除，比如abcddb这种，所以i = max(map[s[j]], i);3. 更新最长的长度: ans = max(ans, j - i + 1);</code></pre><h3 id="最长回文子串-5"><a href="#最长回文子串-5" class="headerlink" title="最长回文子串(5)"></a>最长回文子串(5)</h3><p>将给定字符串反转, 找出原始字符串和反转字符串的最长公共子串(需要做一点小判断).<br>如果已知两个字符串为$X$和$Y$, 且它们各自的长度为$m$和$n$, 那么它们的最长公共后缀的定义如下：</p><script type="math/tex; mode=display">L(m, n)=\left\{\begin{aligned}&  0 &&\text{if}\;\; m = 0 \vee n = 0 \\&  L(m - 1, n - 1) + 1 &&\text{if}\;\; X_{m - 1} = Y_{n - 1} \\&  0 &&\text{if}\;\;X_{m - 1} \neq Y_{n - 1} \\\end{aligned}\right.</script><p>利用上述公式，采用自底向上的动态规划法，保存子串对的最长公共后缀，再从这些后缀中找出最长的那个即可<br>搭出公式对应的框架：</p><pre><code>int[][] dp = new int[m + 1][n + 1];for(int i = 0; i &lt;= m; i ++){    for(int j = 0; j &lt;= n; j ++){        if(i == 0 || j == 0)          dp[i][j] = 0;        else if(X[i - 1] == Y[j - 1]) //对应回原来的数组的索引时i, j分别要减1          dp[i][j] = dp[i - 1][j - 1] + 1;        else          dp[i][j] = 0;    }}</code></pre><pre><code>但是我们要考虑一个问题, 比如给定字符串dabcbe, 反转后为ebcbad,它们的公共子串为：d a b c b ...    2 3 4-&gt;(i - 1)e b c b ...  1 2 3-&gt;(j - 1)假设字符串长度为n(= 6)，满足n - 1 - (j - 1) + dp[i][j] - 1 = 6 - 1 - 3 + 3 - 1 = 4 == (i - 1)但是如果是abcdecba，反转后为abcedcba, 它们的公共子串为：a b c ...0 1 2a b c ...0 1 2不满足n - 1 - (j - 1) + dp[i][j] - 1 == i - 1</code></pre><p>所以在原来的代码的基础上要加入一些判断:</p><pre><code>int maxlen = 0, maxEnd = 0;for(int i = 0; i &lt;= n; i ++){    for(int j = 0; j &lt;= n; j ++) {        if(i == 0 || j == 0)            dp[i][j] = 0;        else if(s.charAt(i - 1) == rs.charAt(j - 1))            dp[i][j] = dp[i - 1][j - 1] + 1;        else            dp[i][j] = 0;        if(dp[i][j] &gt; maxlen) {            int beforeIndex = n - 1 - (j - 1);            if(beforeIndex + dp[i][j] - 1 == i - 1) {                maxlen = dp[i][j];                maxEnd = i - 1;            }        }    }}返回的公共子串的起点根据maxlen = maxEnd - start + 1得到start = maxEnd - maxlen + 1所以返回substring(maxEnd - maxlen + 1, maxEnd + 1)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Leetcode </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客平台搭建</title>
      <link href="/2019/09/03/platform-building/"/>
      <url>/2019/09/03/platform-building/</url>
      
        <content type="html"><![CDATA[<h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>最近博客平台的搭建也告一段落了，再次感谢各位开源大佬，没有你们的支持，我这个小破站也搭建不起来(╯▔皿▔)╯。废话不多说，整个小博客网站主要是借助<a href="https://hexo.io/" target="_blank" rel="noopener">hexo</a>博客框架和<a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank" rel="noopener">3-hexo</a>这个博客主题完成的，3-hexo主题的使用demo可以查看本站下方的友链部分。整个搭建过程真的踩了不少坑，下面我也会分模块尽可能一一讲述清楚。</p><h2 id="hexo搭建步骤"><a href="#hexo搭建步骤" class="headerlink" title="hexo搭建步骤"></a>hexo搭建步骤</h2><h3 id="必要条件"><a href="#必要条件" class="headerlink" title="必要条件"></a>必要条件</h3><ol><li>戳<a href="https://nodejs.org/en/" target="_blank" rel="noopener">nodejs</a>，下载nodejs并安装（本人是在windows上面搭建的），nodejs主要使用npm命令来安装hexo框架的各种插件；</li><li>戳<a href="https://gitforwindows.org/" target="_blank" rel="noopener">git</a>安装windows版git，主要目的是让hexo和github pages配合更新。</li></ol><h3 id="hexo安装"><a href="#hexo安装" class="headerlink" title="hexo安装"></a>hexo安装</h3><p>使用git-bash或者cmd即可安装hexo</p><pre><code>$ npm install -g hexo-cli</code></pre><h3 id="网站配置"><a href="#网站配置" class="headerlink" title="网站配置"></a>网站配置</h3><p>在你指定的存放网站文件的目录下（我的是brightblog，这个目录在后文将被称为hexo根目录）执行下面三行命令：</p><pre><code>$ hexo init brightblog$ cd brightblog$ npm install</code></pre><p>新建完成后可以看到目录下的结构为：</p><pre><code>.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes</code></pre><p>各个目录的主要功能如下所示：</p><div class="table-container"><table><thead><tr><th>文件/目录</th><th>功能</th></tr></thead><tbody><tr><td>_config.xml</td><td>网站的<a href="https://hexo.io/zh-cn/docs/configuration.html" target="_blank" rel="noopener">配置</a>文件</td></tr><tr><td>package.json</td><td>应用程序的信息</td></tr><tr><td>scaffolds</td><td><a href="https://hexo.io/zh-cn/docs/templates.html" target="_blank" rel="noopener">模版</a>文件夹。当您新建文章时，Hexo 会根据scaffold来建立文件</td></tr><tr><td>source</td><td><a href="https://hexo.io/zh-cn/docs/asset-folders.html" target="_blank" rel="noopener">资源文件夹</a>是存放用户资源的地方。除 posts 文件夹之外，开头命名为 (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去</td></tr><tr><td>themes</td><td><a href="https://hexo.io/zh-cn/docs/themes.html" target="_blank" rel="noopener">主题</a>文件夹，Hexo 会根据主题来生成静态页面</td></tr></tbody></table></div><p><br><br>个人认为比较重要的几个文件_config.yml，source目录和themes目录，它们分别是你配置网站，存在博客和存放主题的地方。关于_config.yml，我配置的几个主要地方是：</p><pre><code># 网站标题title: Zeroone# 作者名字author: J. Zhang# 站点的urlurl: https://blog.jtzhang.xyz# 使用的主题，所有的主题都按照名称存放在themes目录下，可根据需要切换theme: hexo-theme-3-hexo# 开启全文搜索(in:)search:  path: search.xml  field: post# 站点部署配置deploy:  - type: git    repo: git@github.com:brightLLer/brightLLer.github.io.git    branch: master</code></pre><h3 id="下载和配置主题"><a href="#下载和配置主题" class="headerlink" title="下载和配置主题"></a>下载和配置主题</h3><p>切换到hexo根目录下的themes文件夹，并执行如下命令下载主题（我使用的是3-hexo主题）：</p><pre><code>$ cd themes/$ git init$ git clone https://github.com/yelog/hexo-theme-3-hexo.git</code></pre><p>下载好的主题的名字要配置在hexo根目录下的_config.yml文件中（见上一小节）。一般来说，一个主题的目录具有如下的文件结构：</p><pre><code>.├── _config.yml├── languages├── layout├── scripts└── source</code></pre><p>主题目录下各个文件/目录的主要功能如下所示:</p><div class="table-container"><table><thead><tr><th>文件/目录</th><th>功能</th></tr></thead><tbody><tr><td>_config.yml</td><td>主题的配置文件</td></tr><tr><td>languages</td><td>语言文件夹，请参见<a href="https://hexo.io/zh-cn/docs/internationalization.html" target="_blank" rel="noopener">国际化 (i18n)</a></td></tr></tbody></table></div><p>layout|布局文件夹。用于存放主题的<a href="https://hexo.io/zh-cn/docs/templates.html" target="_blank" rel="noopener">模板</a>文件，决定了网站内容的呈现方式，您可参考模板以获得更多信息|<br>|scripts|脚本文件夹。在启动时，Hexo 会载入此文件夹内的 JavaScript 文件，参见<a href="https://hexo.io/zh-cn/docs/plugins.html" target="_blank" rel="noopener">插件</a>获取更多信息|<br>|source|资源文件夹，除了模板以外的 Asset，例如 CSS、JavaScript 文件等，都应该放在这个文件夹中。文件或文件夹开头名称为 _（下划线线）或隐藏的文件会被忽略|</p><p><br><br>这里值得一提的是_config.yml文件，这个文件与hexo根目录下的_config.yml文件是不一样的，前者主要用于主题相关的配置，后者则主要是进行网站本身的全局配置。关于主题目录下的_config.yml，我配置的几个主要地方是：</p><pre><code># 你的头像urlavatar: /img/zeroone.pngfavicon: /img/zeroone.png#链接图标 显示顺序和下面顺序一致link:  theme: color # 链接样式，color: 彩色图标  white: 黑白图标  items:    # rss: /atom.xml    github: https://github.com/brightLLer    zhihu: https://www.zhihu.com/people/brightspring/activities    email: 1277472231@qq.com    qq: 1277472231menu:  about:  # &#39;关于&#39; 按钮    on: true # 是否显示    url: /about  # 跳转链接    type: 1 # 跳转类型 1：站内异步跳转 2：当前页面跳转 3：打开新的tab页  friend: # &#39;友链&#39; 按钮    on: true # 是否展示  menus:    github:      on: true      url: https://github.com/brightller      type: 3  # 跳转类型 3：打开新的tab页article_txt: 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 1277472231@qq.combottom_text: ©2019 Jiantao Zhangreward: true# true 开启打赏功能searchAll: true# true 启用全文搜索# 开启此功能需要下面操作：# 1. 在 hexo 根目录 执行 npm install hexo-generator-search --save 安装插件# 2. 在 hexo 根目录的 _config.xml 中添加下面内容# search:#   path: search.xml#   field: postword_count: true# true 开启字数统计# 开启此功能需要安装插件 ：在 hexo根目录 执行npm i hexo-wordcount --savecomment:  on: true  type: gitalk  # 评论系统：gitalk、disqus、gentie、gitment,注意：使用时，在下方对应位置进行配置  comment_count: true  # 文章标题下方显示评论数  preload_comment: false  # 预加载评论区  ## false: 当点击评论条等区域时再加载评论模块  ## true: 页面加载时加载评论区gitalk:  githubID: brightller  repo: brightller.github.io  ClientID:  ClientSecret:  adminUser: brightller  distractionFreeMode: true  language: zh-CN  perPage: 10# gitalk：利用github issue制作的第三方聊天插件# https://github.com/gitalk/gitalk# 参数介绍：# githubID: github用户名# repo: 使用哪个仓库的issue# ClientID 和 ClientSecret 创建 OAuth application 就会生成：https://github.com/settings/applications/new# adminUser: 必须为上面仓库的合作者（有写入权限），使用自己的 github 用户名即可# distractionFreeMode: 全屏遮罩效果# language: 语言：支持：en / zh-CN / zh-TW 三种# perPage: 每次加载的数据大小，默认10，最大100</code></pre><h2 id="站点测试和部署"><a href="#站点测试和部署" class="headerlink" title="站点测试和部署"></a>站点测试和部署</h2><p>在hexo根目录/站点根目录(brightblog)下打开cmd命令行，执行如下命令</p><pre><code>hexo s</code></pre><p>之后再浏览器的地址栏输入<code>http://localhost:4000</code>就可以看到测试的站点，如下图所示，按住<code>Ctrl+C</code>可以退出本地测试。<br><img src="/img/local-test.png"><br>如果检查后觉得测试页面没有什么问题，就可以部署到Github pages上面了。</p><ol><li>首先在git-bash上执行如下命令安装部署hexo站点用的插件：<pre><code>npm install hexo-deployer-git --save</code></pre></li><li>在Github上创建名为<code>&lt;username&gt;.github.io</code>的仓库，比如我的就是<code>brightller.github.io</code></li><li>在网站的配置文件（不是主题的配置文件）_config.yml中填好部署的配置：<pre><code>deploy:- type: git repo: git@github.com:brightLLer/brightLLer.github.io.git branch: master</code></pre></li><li><p>一般来说,github提交代码有两种方式，一种为HTTPS方式，另一种为SSH方式：</p><pre><code># HTTPS方式:https://github.com/brightLLer/brightLLer.github.io.git# SSH方式:git@git.com:brightLLer/brightLLer.github.io.git</code></pre><p>以HTTPS方式提交代码需要每次手动输入github的账号密码，而SSH方式是将本地系统的公钥(每换一台电脑就要重新生成一次)提供给github，之后就不需要每次手动输入账号和密码了，回头看第3点，由于部署的方式是SSH方式，因此我们需要先生成SSH密钥：</p><pre><code>ssh-keygen -t rsa -C &quot;1277472231@qq.com&quot;</code></pre><p>接着切换到<code>C:\Users\&lt;local_user&gt;\.ssh</code>下，可以看到目录结构：</p><pre><code>.├── id_rsa├── id_rsa.pub└── known_hosts</code></pre><p>将id_rsa.pub中的公钥复制到github上面：</p><pre><code>头像 -&gt; settings -&gt; SSH and GPG keys -&gt; New SSH key -&gt; 填写title(随意)和key</code></pre><p><img src="/img/SSH-github.png"></p></li><li><p>关联本地仓库和远程仓库:</p><pre><code>git remote add origin git@git.com:brightLLer/brightLLer.github.io.git# 或者git remote add origin https://github.com/brightLLer/brightLLer.github.io.git# 可缺省https链接</code></pre></li><li><p>最终部署：</p><pre><code># 第一次部署hexo deploy# 非第一次部署（修改过内容后重新部署，如主题）hexo cleanhexo generatehexo deploy</code></pre></li></ol><p>除此之外，为了开启评论系统Gitalk，我们需要注册<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">OAuth Application</a>，进去之后我们可以看到下方的界面：<br><img src="/img/OAuth-App.png"><br>这里<code>https://blog.jtzhang.xyz/</code>是我申请的域名，如果没有域名就还是填原来的<code>https://&lt;username&gt;.github.io/</code>就可以了，注册好之后我们会得到自己的<code>client_id</code>和<code>client_secret</code>，可以在github中采用如下方式进入查看：</p><pre><code>头像 -&gt; Developer settings -&gt; OAuth Apps -&gt; App头像</code></pre><p>将它们填写到主题配置文件_config.yml（注意不是网站配置文件_config.yml）上对应的位置就可以了：</p><pre><code>gitalk:  githubID: brightller  repo: brightller.github.io  ClientID: your client_id  ClientSecret: your client_secret  adminUser: brightller  distractionFreeMode: true  language: zh-CN  perPage: 10</code></pre><h2 id="申请自己的域名"><a href="#申请自己的域名" class="headerlink" title="申请自己的域名"></a>申请自己的域名</h2><p>由于我使用的是腾讯云，所以将腾讯云作为例子，其它平台的操作也是类似的。</p><ol><li>注册腾讯云账号，登录，进入控制台，购买域名，域名大部分需要实名认证，上传自己的身份证即可，审核一般当天能完成.</li><li>进入域名管理，如果审核通过了，可以看到：<br><img src="/img/domain-manage.png"></li><li>点击上图中的解析，进入之后点击添加如下3条记录，其中前两条是固定的，解析的是github的IP地址，最后一条请填写你自己的仓库名<code>&lt;username&gt;.github.io</code>，主机记录可以随意填，既然是博客可以采用blog这样的一个二级域名，记录类型建议填写CNAME，不要填A，这样github仓库待24小时内审核之后可以直接勾上enfore https，这样网站就可以安全访问了：<br><img src="/img/all-records.png"><br><img src="/img/enforce-https.png"></li><li>在hexo根目录的source文件夹下建立一个名为CNAME的文件(不需要扩展名)，然后在里面写上申请的域名，比如我的就是blog.jtzhang.xyz</li><li>重新clean，generate，deploy，就可以直接使用域名访问自己的网站了.</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://hexo.io/" target="_blank" rel="noopener">https://hexo.io/</a></p><p><a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank" rel="noopener">https://github.com/yelog/hexo-theme-3-hexo</a></p><p><a href="https://www.yuque.com/skyrin/coding/tm8yf5" target="_blank" rel="noopener">https://www.yuque.com/skyrin/coding/tm8yf5</a></p><p><a href="https://imsun.net/posts/gitment-introduction/" target="_blank" rel="noopener">https://imsun.net/posts/gitment-introduction/</a></p><p><a href="https://www.cnblogs.com/liuurick/p/10713687.html" target="_blank" rel="noopener">https://www.cnblogs.com/liuurick/p/10713687.html</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> 3-hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>librosa术语介绍</title>
      <link href="/2019/09/02/librosa-term/"/>
      <url>/2019/09/02/librosa-term/</url>
      
        <content type="html"><![CDATA[<h1 id="librosa中的相关术语介绍"><a href="#librosa中的相关术语介绍" class="headerlink" title="librosa中的相关术语介绍"></a>librosa中的相关术语介绍</h1><blockquote><p>注：librosa是一个音频处理库，戳<a href="http://librosa.github.io/librosa/" target="_blank" rel="noopener">这里</a>看官方document。</p></blockquote><h2 id="术语解释"><a href="#术语解释" class="headerlink" title="术语解释"></a>术语解释</h2><h3 id="time-series"><a href="#time-series" class="headerlink" title="time series"></a><strong>time series</strong></h3><p>Typically an audio signal, denoted by <code>y</code>, and represented as a one-dimensional numpy.ndarray of floating-point values. <code>y[t]</code> corresponds to amplitude of the waveform at sample <code>t</code>.<br>时间序列：一般指的是音频信号，用<code>y</code>来表示，<code>y</code>是一个浮点类型的一维数组，<code>y[t]</code>表示波形图中样本<code>t</code>处的幅度值</p><h3 id="sampling-rate"><a href="#sampling-rate" class="headerlink" title="sampling rate"></a><strong>sampling rate</strong></h3><p>The (positive integer) number of samples per second of a time series. This is denoted by an integer variable <code>sr</code>.<br>采样频率：时间序列中，每秒的样本数目（一个正整数）。经常用一个整型变量<code>sr</code>来表示</p><h3 id="frame"><a href="#frame" class="headerlink" title="frame"></a><strong>frame</strong></h3><p>A short slice of a time series used for analysis purposes. This usually corresponds to a single column of a spectrogram matrix.<br>帧：一个时间序列的短切片，常用于分析。帧通常与时频矩阵中一个单独的列相关（specgram的行表示频率，列表示时间），强调一点，帧的意义是时间上的。</p><h3 id="window"><a href="#window" class="headerlink" title="window"></a><strong>window</strong></h3><p>The (positive integer) number of samples in an analysis window (or frame). This is denoted by an integer variable <code>n_fft</code>.<br>在一个分析窗（或者帧）内的样本的数目（一个正整数）。用整型变量<code>n_fft</code>来表示。<br>注：1、傅里叶变换只能对作用于有限长度的信号，因此才要对信号采样，采样数目<code>n_fft</code>就是一帧的长度<br>2、帧的中心样本称为center，<code>stft</code>等函数中，<code>center</code>参数若为<code>True</code>，信号y会被左右填充(padded)以确保第<code>k</code>帧以<code>k*hop_length</code>那个样本为中心（v3.0开始），若为<code>False</code>，则不填充而且第k帧的起点就是<code>k*hop_length</code>那个样本。对应到specgram中就是<code>D[:, t]</code>或者<code>S[:, t]</code><br>center : boolean</p><ul><li>If <code>True</code>, the signal <code>y</code> is padded so that frame <code>D[:, t]</code> is centered at <code>y[t * hop_length]</code>.</li><li>If <code>False</code>, then <code>D[:, t]</code> begins at <code>y[t * hop_length]</code><h3 id="hop-length"><a href="#hop-length" class="headerlink" title="hop length"></a><strong>hop length</strong></h3>The number of samples between successive frames, e.g., the columns of a spectrogram. This is denoted as a positive integer <code>hop_length</code>.<br>帧移：连续帧之间的样本数目，用一个正整数变量<code>hop_length</code>来表示。注：其实就是上一帧的起点到下一帧起点的位移<h3 id="window-length"><a href="#window-length" class="headerlink" title="window length"></a><strong>window length</strong></h3>The length (width) of the window function (e.g., Hann window). Note that this can be smaller than the frame length used in a short-time Fourier transform. Typically denoted as a positive integer variable <code>win_length</code>.<br>窗长度：窗的长度（宽度），在短时傅里叶变换中可以比帧长度小（多数时候两者长度一样，窗长小于帧长时使用零填充补充至长度相等）。经常用正整数变量<code>win_length</code>来表示<h3 id="spectrogram"><a href="#spectrogram" class="headerlink" title="spectrogram"></a><strong>spectrogram</strong></h3>A matrix <code>S</code> where the rows index frequency bins, and the columns index frames (time). Spectrograms can be either real-valued or complex-valued. By convention, real-valued spectrograms are denoted as numpy.ndarrays <code>S</code>, while complex-valued STFT matrices are denoted as <code>D</code>.<br>时频矩阵：一个矩阵，其行索引频带，列索引帧（时间）。时频矩阵的元素可以是实数也可以是复数。按照约定俗成的表示，实数时频矩阵用数组<code>S</code>表示，而复数STFT矩阵则用<code>D</code>表示<br><code>np.abs(D[f, t])</code> is the magnitude of frequency bin <code>f</code> at frame <code>t</code><br><code>np.angle(D[f, t])</code> is the phase of frequency bin <code>f</code> at frame <code>t</code><h3 id="onset-strength-envelope"><a href="#onset-strength-envelope" class="headerlink" title="onset (strength) envelope"></a><strong>onset (strength) envelope</strong></h3>An onset envelope <code>onset_env[t]</code> measures the strength of note onsets at frame <code>t</code>. Typically stored as a one-dimensional numpy.ndarray of floating-point values onset_envelope.<br>帧起点数组：记录着每一帧起始时刻处的强度，用一个浮点型一维数组<code>onset_envelope</code>来表示</li></ul>]]></content>
      
      
      <categories>
          
          <category> 语音分离 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> librosa </tag>
            
            <tag> signal processing </tag>
            
            <tag> speech separation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>librosa补充(一)</title>
      <link href="/2019/09/02/librosa-additional/"/>
      <url>/2019/09/02/librosa-additional/</url>
      
        <content type="html"><![CDATA[<h1 id="补充几个概念以及对部分API的理解"><a href="#补充几个概念以及对部分API的理解" class="headerlink" title="补充几个概念以及对部分API的理解"></a>补充几个概念以及对部分API的理解</h1><blockquote><p>注：时域信号在经过傅里叶变换转换之后一般为复数形式</p></blockquote><h2 id="amplitude和magnitude的区别"><a href="#amplitude和magnitude的区别" class="headerlink" title="amplitude和magnitude的区别"></a><strong>amplitude和magnitude的区别</strong></h2><ul><li>peak amplitude, often shortened to amplitude, is the nonnegative value of the waveform’s peak (either positive or negative).(强调信号的峰值)</li><li>instantaneous amplitude of x is the value of x(t) (either positive or negative) at time t.（瞬时幅度，信号x(t)在t处的值）</li><li>instantaneous magnitude, or simply magnitude, of x is nonnegative and is given by |x(t)|.（瞬时幅度，信号x(t)的模）<br>需要特意强调的就是，magnitude指的是模，不过在多数情况下，amplitude和magnitude在fft中一般是同一个意思。</li></ul><h2 id="振幅，分贝-dB-和能量-power-之间的转换关系"><a href="#振幅，分贝-dB-和能量-power-之间的转换关系" class="headerlink" title="振幅，分贝(dB)和能量(power)之间的转换关系"></a><strong>振幅，分贝(dB)和能量(power)之间的转换关系</strong></h2><ul><li>振幅：由于傅里叶变换之后得到的为复数T-F矩阵D，在python里，可以简单得到振幅的T-F矩阵和相位(幅角)的T-F矩阵：<pre><code>S = np.abs(D)P = np.angle(D)</code></pre></li><li><p>分贝：librosa的官方文档中对分贝的计算公式定义如下：</p><pre><code>S_db = 20 * np.log10(S / ref)</code></pre><p>其中ref的默认取值为1.0，不过文档似乎更习惯于用<code>ref = np.max(S)</code></p></li><li><p>能量：T-F矩阵中每个T-F单元的能量就是幅值取平方即可</p><pre><code>S_power = S ** 2</code></pre><h2 id="librosa中xx-to-xx型的API"><a href="#librosa中xx-to-xx型的API" class="headerlink" title="librosa中xx_to_xx型的API"></a><strong>librosa中xx_to_xx型的API</strong></h2></li><li><p><code>amplitude_to_db(S[, ref, amin, top_db])</code><br>该API是将幅值转换为分贝，这里的第一个参数S，按照librosa相关术语的描述，应该是一个实数T-F矩阵而不是复数T-F矩阵，但实际测试的时候发现API传入S和D的效果是一样的，源码内部实际上有一句<code>magnitude = np.abs(S)</code>，那么说明参数中的S实际上是被当成D来看的，D中的每个瞬时值被称为amplitude，不过从字面意义上来看，还是当成实数T-F矩阵S来看比较好，也就是说时域信号<code>y</code>经过STFT变换后得到<code>D</code>，先经过<code>S = np.abs(D)</code>之后再传入该API中的S，当然直接传入<code>D</code>也没有任何问题。此外该<code>amplitude_to_db(S)</code>与<code>power_to_db(S ** 2)</code>等价。</p></li><li><p><code>db_to_amplitude(S_db[, ref])</code><br>将分贝转换为幅值。这里的幅值是实数，用magnitude更合适，强调模S。</p></li><li><p><code>power_to_db(S[, ref, amin, top_db, ref_power])</code><br>能量转换为分贝，ref_power参数会在0.6.0版本起抛弃。</p></li><li><p><code>db_to_power(S_db[, ref])</code><br>分贝转换为能量。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 语音分离 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> librosa </tag>
            
            <tag> signal processing </tag>
            
            <tag> speech separation </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
